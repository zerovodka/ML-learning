{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP/qJdHM2Tb9outXOGedF1P"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# BERT vs GPT Tokenizer and AutoTokenizer\n",
        "- 이모지 처리 관점에서 비교\n",
        "- 모델에 맞는 tokenizer를 자동으로 불러오는 **`AutoTokenizer`**까지 알아보자"
      ],
      "metadata": {
        "id": "AQ1luoKUrtCh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BERT Tokenizer"
      ],
      "metadata": {
        "id": "A-9wqD-5yhna"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mHlUdeR4mSNd",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1743517260691,
          "user_tz": -540,
          "elapsed": 6383,
          "user": {
            "displayName": "표표표표푱",
            "userId": "15791434277334549064"
          }
        }
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizerFast\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이모지를 넣어보자\n",
        "- BERT: 이모지 데이터 -> [UNK] 처리: 데이터의 완전 손실 -> 디코딩 시 이모지 데이터는 없음\n",
        "- GPT: 이모지 데이터 -> 이모지를 유니코드 문자 그대로 취급하여 손실 없음 / 이모지를 별도로 전처리해서 토큰화도 가능"
      ],
      "metadata": {
        "id": "eu9GZWkWuuGv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# BERT tokenizer 불러오기\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# 예제 text\n",
        "text = 'This movie was incredibly touching and had great performances!! I would definitely recommend it to anyone 🤔🤔.'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254,
          "referenced_widgets": [
            "e2906a873ab24767a00ac50f683ff5dd",
            "81717af753f641cb89d4ac8c0355378f",
            "691ab72c73f84b409a3141230f2d2f5b",
            "999c568811a245cb93fd5822ff646c5d",
            "958335bcde7f4ba6ac433e673d13cbcb",
            "409f49ff43c14f248eb40fd029a53b82",
            "bdcde7de8026442c974b7c3e739e3b4f",
            "63ba12936f9843ea835c233022208f3e",
            "2dd33b58eb994863ab63706218adc230",
            "d97df96a25f9444aa8420ddbebacd946",
            "cb8514a40874469da66142b13173448b",
            "cee0233318d940cc8e062a92ba531c54",
            "df6064039a774c05ae31ac9a381738db",
            "b785a44b7e524706afb558de423a06ef",
            "9dd06adb72764091ac0dd7ebbf2dfe2e",
            "9463f7487b15488d8cb1280163a0a9f8",
            "fc6871346af5441c84c9a7fef76b4c85",
            "df0ee36a0db542f6889db9ffadf0ff96",
            "dea0f7187cff4965a40c218b6ca4bfa0",
            "276cd742d1ca401a806a679be0760267",
            "8fdc4ec7b12c4b3c9ab585968ef5a5c3",
            "15e0e39552c449cca984793259202a5e",
            "7d5e07f8eb8f4fe79ed6049a9e6838e2",
            "763957b55d5841c4bd7d405a0a4f300c",
            "7ea939ac83904166bf23a8d35e735e0a",
            "6a503fa1ad6d44d78f9fd138a4cf9cb4",
            "006e20a0f2674c309991554dd899c475",
            "b9c1a5aecfb248c0bccf5da5a0203d8e",
            "1721f0c08f9d40ddaf9c34fc199aecf2",
            "89cdb298050841a5b6f31b513db634fb",
            "66c3d840dfea48e4873236cf01fe1753",
            "e3a450d0374c4822b8770aee20c015f8",
            "b934f24852cb41309e61ba659576f139",
            "cad39f9ad5a84327aac8beffb6c97b92",
            "b5bd3f155c084fabb54b3bddd7ca23a3",
            "894cbada39b14e4b9913ea3b3f8ebc5c",
            "8484cdace7fc44d7989ba5f96d24e93b",
            "de564af199754843a28e94adab18d74e",
            "e89ce3680ecf40bbbf8d0e7e9639065a",
            "3b061ff9cfb8479781205a5afdfcbaea",
            "ecfca5b63b054fba93b3c6d96ad37c48",
            "91546290abc84ba4ac1259fe05ff72d2",
            "1ff77ae845124332874946b164bed246",
            "b0b8dde7371e4872a82c59f34ad0094b"
          ]
        },
        "id": "Wn3Ex6FKntCW",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1743517273614,
          "user_tz": -540,
          "elapsed": 10110,
          "user": {
            "displayName": "표표표표푱",
            "userId": "15791434277334549064"
          }
        },
        "outputId": "e656348f-026e-49cc-8000-b7e5f7c0f8da"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e2906a873ab24767a00ac50f683ff5dd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cee0233318d940cc8e062a92ba531c54"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7d5e07f8eb8f4fe79ed6049a9e6838e2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cad39f9ad5a84327aac8beffb6c97b92"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer 동작 확인\n",
        "encoded = tokenizer(\n",
        "    text,\n",
        "    padding='max_length', # padding\n",
        "    truncation=True,      # 너무 길면 자르기\n",
        "    max_length=30,        # 최대 길이 20 제한\n",
        "    return_tensors='pt'   # PyTorch 텐서로 변환\n",
        ")\n",
        "\n",
        "tokens_bert = tokenizer.tokenize(text)\n",
        "input_ids_bert = tokenizer.encode(text)\n",
        "decoded_bert = tokenizer.decode(input_ids_bert)"
      ],
      "metadata": {
        "id": "Ih7-uvTsoJMd",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1743518813353,
          "user_tz": -540,
          "elapsed": 2808,
          "user": {
            "displayName": "표표표표푱",
            "userId": "15791434277334549064"
          }
        }
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERT Tokenizer 출력 결과"
      ],
      "metadata": {
        "id": "UMIDwuTF3REW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'입력된 문장: {text}')\n",
        "\n",
        "print(\"\\nTokenized input_ids:\")\n",
        "print(encoded[\"input_ids\"])\n",
        "\n",
        "print(\"\\nTokenized attention_mask:\")\n",
        "print(encoded[\"attention_mask\"])\n",
        "\n",
        "print(\"\\nLongTensor 타입 확인:\")\n",
        "print(type(encoded[\"input_ids\"]))\n",
        "\n",
        "print(\"\\nDecode:\")\n",
        "print(tokenizer.decode(encoded['input_ids'][0]))\n",
        "\n",
        "print(\"\\nTokens:\")\n",
        "print(tokens_bert)\n",
        "\n",
        "print(\"\\ninput_ids:\")\n",
        "print(input_ids_bert)\n",
        "\n",
        "print(\"\\nDecoded input_ids:\")\n",
        "print(decoded_bert)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3j3igOBocGe",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1743518882995,
          "user_tz": -540,
          "elapsed": 15,
          "user": {
            "displayName": "표표표표푱",
            "userId": "15791434277334549064"
          }
        },
        "outputId": "d0079b49-559c-4fce-f6dd-cc0e59d1882f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력된 문장: This movie was incredibly touching and had great performances!! I would definitely recommend it to anyone 🤔🤔.\n",
            "\n",
            "Tokenized input_ids:\n",
            "tensor([[  101,  2023,  3185,  2001, 11757,  7244,  1998,  2018,  2307,  4616,\n",
            "           999,   999,  1045,  2052,  5791, 16755,  2009,  2000,  3087,   100,\n",
            "          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0]])\n",
            "\n",
            "Tokenized attention_mask:\n",
            "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0]])\n",
            "\n",
            "LongTensor 타입 확인:\n",
            "<class 'torch.Tensor'>\n",
            "\n",
            "Decode:\n",
            "[CLS] this movie was incredibly touching and had great performances!! i would definitely recommend it to anyone [UNK]. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "\n",
            "Tokens:\n",
            "['this', 'movie', 'was', 'incredibly', 'touching', 'and', 'had', 'great', 'performances', '!', '!', 'i', 'would', 'definitely', 'recommend', 'it', 'to', 'anyone', '[UNK]', '.']\n",
            "\n",
            "input_ids:\n",
            "[101, 2023, 3185, 2001, 11757, 7244, 1998, 2018, 2307, 4616, 999, 999, 1045, 2052, 5791, 16755, 2009, 2000, 3087, 100, 1012, 102]\n",
            "\n",
            "Decoded input_ids:\n",
            "[CLS] this movie was incredibly touching and had great performances!! i would definitely recommend it to anyone [UNK]. [SEP]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "|항목|설명|\n",
        "|:---|:----------------------------|\n",
        "|`input_ids`|\t토크나이징된 BERT용 숫자 시퀀스|\n",
        "|`attention_mask`|\t실제 단어는 1, padding은 0|\n",
        "|길이|\t`max_length=20`으로 제한되었기 때문에 20개|\n",
        "|`type`|\t`<class 'torch.Tensor'>` → LongTensor로 자동 변환|"
      ],
      "metadata": {
        "id": "KFtQRyQpo7rP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenized `input_ids`\n",
        "```\n",
        "tensor([\n",
        "  [101, 2023, 3185, 2001, 11757, 7244, 1998, 2018, 2307, 4616,\n",
        "   999,  999, 1045, 2052, 5791, 16755, 2009, 2000, 3087, 100,\n",
        "   1012, 102, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "])\n",
        "```\n",
        "\n",
        "|토큰 ID|\t의미|\n",
        "|:---|:---|\n",
        "|101|\t[CLS] → 문장의 시작|\n",
        "|100|\t[UNK] → 🤔 이모지를 토크나이저가 모름 → \"Unknown\" 토큰으로 대체 👉🏻 이모지 2개를 하나로 퉁침|\n",
        "|1012|\t. 마침표|\n",
        "|102|\t[SEP] → 문장의 끝|\n",
        "|0|\t[PAD] → max_length 30에 맞춰 빈자리를 채움|\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "### Tokenized `attention_mask`\n",
        "```\n",
        "tensor([\n",
        "  [1, 1, 1, ..., 1, 1, 0, 0, 0, ..., 0]\n",
        "])\n",
        "```\n",
        "- 1: 실제 단어 또는 특수 토큰 ([CLS], [SEP])\n",
        "- 0: padding ([PAD] 자리)   \n",
        "  → 모델이 학습할 때 **0인 위치는 무시**\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "### Decoding Result\n",
        "```\n",
        "[CLS] this movie was incredibly touching and had great performances!! i would definitely recommend it to anyone [UNK]. [SEP] [PAD] [PAD] ...\n",
        "```\n",
        "- `tokenizer.decode(input_ids)`: token화 된 input tensor를 다시 NL로 변환"
      ],
      "metadata": {
        "id": "N9BpcOkCu2K1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPT Tokenizer"
      ],
      "metadata": {
        "id": "JMSQESw4ynap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2TokenizerFast"
      ],
      "metadata": {
        "id": "9pwJ26weyovh",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1743518364220,
          "user_tz": -540,
          "elapsed": 52,
          "user": {
            "displayName": "표표표표푱",
            "userId": "15791434277334549064"
          }
        }
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "실습 text는 위에 선언한 text 그대로 사용\n",
        "- 정확한 비교를 위함"
      ],
      "metadata": {
        "id": "emy2ZJUEy8wb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GTP2 토크나이저 불러오기\n",
        "tokenizer_gpt = GPT2TokenizerFast.from_pretrained(\"gpt2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177,
          "referenced_widgets": [
            "4c7dc6e79d6544b68091d431ca98db2a",
            "0827a7b53f3b4cc4bfffc2ebd01bf2d4",
            "7cc2417e89f946f28de476cb7b41ba77",
            "4cf59125f02448cf995170a455060a17",
            "a1e223cdb9eb49bb9587ef744d50d944",
            "c5e22180453249eb996d0a50399ec5fc",
            "191fea53e1fd4c498e8189d0eb8a43bc",
            "7a873e39a7cd42bbbe9e67eb9df7706b",
            "ef045c3c761d417ebf36153143b3a09c",
            "21cc793b3a5e4b2a9694bb59afd0a188",
            "389865c2031d47829431c46265287a7b",
            "c5bb0c64011445dea07b25ed9d3b9256",
            "9074db6133a24951a9854462a372074d",
            "504d8d239a0c4c75af7fe4bf6e4add1e",
            "448fd463c57148eb8f22d0fdca90767d",
            "accd9301467543909449477f4a0c955c",
            "8f18472538a44954a3d9de5f21f5176c",
            "3b5beffb02824b23948ae02ee34ca269",
            "9dd676b7ba7c401ba5567db423b584d5",
            "8b6ef9946d2a4f0da11d30c86bea0f64",
            "78ff8e1854284522895dd9c3ce440c49",
            "c7d31ae1f33445debbd78975d77b5ced",
            "ff465280c873440e835d4aa1fc01dc69",
            "92077e60fb1e4036a47a2b1136b1c809",
            "1eeed87f9ad7437da193b8d1e59fdc63",
            "2b08112402eb4dfe88328bcaba8b663a",
            "d1c01fb236674938a5b77418c1d56228",
            "b716bc71f4c94384bf55887a0d154ca1",
            "1afd6ddbcae34d8cb35460f8c255fd9d",
            "131e319fcf47411594da5873cf0c0b23",
            "31824b6d7fc54ad69e4649c68b395c08",
            "ab9000c086394ebc898a186c0c6cac36",
            "393e7738376242ddbc932b6b5cd73a49",
            "87c727683dcb48fdb766677c174bc161",
            "8fc8332b3b40488fb1fa28321e62c9e1",
            "b1401d5cd9b546418d5a93bb220f6ad0",
            "8b884ccea28c494dbee34fd53b5bd7ef",
            "f83545537d514e0facd29c00675d86ce",
            "39b327380e1f456ab8323082dbc5296e",
            "1704527bc7d9437ca64b7ca8cd5d8d89",
            "f961a7122174413b9c2eb69a1f80cadd",
            "31dccaec27764704a0a2ca593e1f120b",
            "af70c4e6ca2a48ffacb768caf58b3521",
            "77793944a9dc4f34a8b5bde85df3dcc2",
            "1c2eb95d63a4419683241c6777ddabbf",
            "4bca93bd2db041c7843bae7b9c8559b4",
            "91f2e649c0e845ea879f9ea80ec95140",
            "c2a39e59ffe34803b6884b888c099257",
            "caffb7e694bb4a81a3317b927f2f7bb2",
            "8b8e41c3c6cc43a29b6a83c544b75dfb",
            "1635b81fde6c48d6aa8702a69ff2ad89",
            "9ba0221090cd4517844f093c8f86efb9",
            "a91c302daa3e4efeb58856c13f0202d3",
            "5e5b13b809504f97a9923e22761910d8",
            "4d6afe37482a47f8aa63a4de14cf6834"
          ]
        },
        "id": "khIkKWc_yvCQ",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1743518419385,
          "user_tz": -540,
          "elapsed": 2669,
          "user": {
            "displayName": "표표표표푱",
            "userId": "15791434277334549064"
          }
        },
        "outputId": "993599ca-9b09-4078-c82b-6eb2b11ee36a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4c7dc6e79d6544b68091d431ca98db2a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c5bb0c64011445dea07b25ed9d3b9256"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ff465280c873440e835d4aa1fc01dc69"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "87c727683dcb48fdb766677c174bc161"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1c2eb95d63a4419683241c6777ddabbf"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GPT는 padding을 지원하지 않는다**   \n",
        "- GPT는 **문장 생성용 모델**이기 때문에 한 문장씩 **오른쪽으로만 길어지는** 구조\n",
        "  ```\n",
        "  GPT는 원래 padding이 필요하지 않다\n",
        "  ```\n",
        "- BERT는 문장 분류용으로 **패딩 + 마스킹**이 일반적이다\n",
        "- GPT2 Token에 pad_token을 명시적으로 추가하면 padding 옵션도 정상 작동하긴 하는데 굳이 느낌임\n",
        "  ``` python\n",
        "  # 이런 식으로 하면 됨\n",
        "  tokenizer_gpt.pad_token = tokenizer_gpt.eos_token\n",
        "  ```\n",
        "\n",
        "  <br>\n",
        "\n",
        "**Summary**   \n",
        "\n",
        "|원인|해결 방법|\n",
        "|:---|:---|\n",
        "|GPT tokenizer는 기본적으로 `pad_token` 없음|\t`tokenizer.pad_token = tokenizer.eos_token` 으로 지정|\n",
        "|padding='max_length' 사용 시 에러 발생 가능|위처럼 `pad_token`을 설정하면 해결|\n",
        "|BERT와 GPT는 토크나이저 기본 설정이 다름|`AutoTokenizer`를 사용하거나 커스터마이즈 필요|"
      ],
      "metadata": {
        "id": "RHppsyYh1-s7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_gpt = tokenizer_gpt(\n",
        "    text,\n",
        "    # padding='max_length', # padding\n",
        "    truncation=True,      # 너무 길면 자르기\n",
        "    max_length=30,        # 최대 길이 20 제한\n",
        "    return_tensors='pt'   # PyTorch 텐서로 변환\n",
        ")"
      ],
      "metadata": {
        "id": "CNIF8-FUy74Q",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1743519008279,
          "user_tz": -540,
          "elapsed": 2,
          "user": {
            "displayName": "표표표표푱",
            "userId": "15791434277334549064"
          }
        }
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_gpt = tokenizer_gpt.tokenize(text)\n",
        "input_ids_gpt = tokenizer_gpt.encode(text)\n",
        "decoded_gpt = tokenizer_gpt.decode(input_ids_gpt)"
      ],
      "metadata": {
        "id": "lw8mirVrzV6I",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1743519176775,
          "user_tz": -540,
          "elapsed": 5,
          "user": {
            "displayName": "표표표표푱",
            "userId": "15791434277334549064"
          }
        }
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPT Tokenizer 출력 결과"
      ],
      "metadata": {
        "id": "iRPwsIfo3VaJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'입력된 문장: {text}')\n",
        "\n",
        "print(\"\\nTokenized input_ids:\")\n",
        "print(encoded_gpt[\"input_ids\"])\n",
        "\n",
        "print(\"\\nTokenized attention_mask:\")\n",
        "print(encoded_gpt[\"attention_mask\"])\n",
        "\n",
        "print(\"\\nLongTensor 타입 확인:\")\n",
        "print(type(encoded_gpt[\"input_ids\"]))\n",
        "\n",
        "print(\"\\nDecode:\")\n",
        "print(tokenizer_gpt.decode(encoded_gpt['input_ids'][0]))\n",
        "\n",
        "print(\"\\nTokens:\")\n",
        "print(tokens_gpt)\n",
        "\n",
        "print(\"\\ninput_ids:\")\n",
        "print(input_ids_gpt)\n",
        "\n",
        "print(\"\\nDecoded input_ids:\")\n",
        "print(decoded_gpt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2YtsC7V1UCI",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1743519190087,
          "user_tz": -540,
          "elapsed": 52,
          "user": {
            "displayName": "표표표표푱",
            "userId": "15791434277334549064"
          }
        },
        "outputId": "dd4fb531-5730-48e1-8180-8bd2bd2d6dd9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력된 문장: This movie was incredibly touching and had great performances!! I would definitely recommend it to anyone 🤔🤔.\n",
            "\n",
            "Tokenized input_ids:\n",
            "tensor([[ 1212,  3807,   373,  8131, 15241,   290,   550,  1049, 13289,  3228,\n",
            "           314,   561,  4753,  4313,   340,   284,  2687, 12520,    97,   242,\n",
            "          8582,    97,   242,    13]])\n",
            "\n",
            "Tokenized attention_mask:\n",
            "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
            "\n",
            "LongTensor 타입 확인:\n",
            "<class 'torch.Tensor'>\n",
            "\n",
            "Decode:\n",
            "This movie was incredibly touching and had great performances!! I would definitely recommend it to anyone 🤔🤔.\n",
            "\n",
            "Tokens:\n",
            "['This', 'Ġmovie', 'Ġwas', 'Ġincredibly', 'Ġtouching', 'Ġand', 'Ġhad', 'Ġgreat', 'Ġperformances', '!!', 'ĠI', 'Ġwould', 'Ġdefinitely', 'Ġrecommend', 'Ġit', 'Ġto', 'Ġanyone', 'ĠðŁ', '¤', 'Ķ', 'ðŁ', '¤', 'Ķ', '.']\n",
            "\n",
            "input_ids:\n",
            "[1212, 3807, 373, 8131, 15241, 290, 550, 1049, 13289, 3228, 314, 561, 4753, 4313, 340, 284, 2687, 12520, 97, 242, 8582, 97, 242, 13]\n",
            "\n",
            "Decoded input_ids:\n",
            "This movie was incredibly touching and had great performances!! I would definitely recommend it to anyone 🤔🤔.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AutoTokenizer"
      ],
      "metadata": {
        "id": "WYl4P59Q4lLj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "V-d1xflx4pNL",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1743519941561,
          "user_tz": -540,
          "elapsed": 5,
          "user": {
            "displayName": "표표표표푱",
            "userId": "15791434277334549064"
          }
        }
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AutoTokenizer의 효과\n",
        "- 코드 재사용성이 높아진다\n",
        "  ``` python\n",
        "  from transformers import BertTokenizerFast\n",
        "  from transformers import GPT2TokenizerFast\n",
        "  ```\n",
        "  기존에는 위 코드처럼 tokenizer를 각각 import\n",
        "\n",
        "- 하단과 같이 코드를 `AutoTokenizer`로 묶게되면, 그에 상응하는 tokenizer를 바로 사용할 수 있다\n",
        "  - **import를 AutoTokenizer 하나만 하면 되게 되는 것**\n",
        "\n",
        "### 한계점\n",
        "- AutoTokenizer는 코드 재사용성을 위함일 뿐\n",
        "- 테스트 환경에서 사용을 지향하고, 프로덕트 배포 시에는 정확한 Tokenizer를 import해서 사용하는 게 성능/안정성 면에서 좋"
      ],
      "metadata": {
        "id": "Ts_ZxQuO6Bde"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_auto_gpt = AutoTokenizer.from_pretrained('gpt2')\n",
        "tokenizer_auto_bert = AutoTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "ZLV-oMXV4wJ0",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1743519985058,
          "user_tz": -540,
          "elapsed": 437,
          "user": {
            "displayName": "표표표표푱",
            "userId": "15791434277334549064"
          }
        }
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_auto_gpt = tokenizer_auto_gpt(\n",
        "    text,\n",
        "    # padding='max_length', # GPT는 padding 지원하지 않는다\n",
        "    truncation=True,      # 너무 길면 자르기\n",
        "    max_length=30,        # 최대 길이 20 제한\n",
        "    return_tensors='pt'   # PyTorch 텐서로 변환\n",
        ")\n",
        "\n",
        "encoded_auto_bert = tokenizer_auto_bert(\n",
        "    text,\n",
        "    padding='max_length', # padding\n",
        "    truncation=True,      # 너무 길면 자르기\n",
        "    max_length=30,        # 최대 길이 20 제한\n",
        "    return_tensors='pt'   # PyTorch 텐서로 변환\n",
        ")"
      ],
      "metadata": {
        "id": "uXgHvvPN46qi",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1743521924628,
          "user_tz": -540,
          "elapsed": 14,
          "user": {
            "displayName": "표표표표푱",
            "userId": "15791434277334549064"
          }
        }
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AutoTokenize - BERT Tokenizer"
      ],
      "metadata": {
        "id": "0K55ArfEAO5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'입력된 문장: {text}')\n",
        "\n",
        "print(\"\\nTokenized input_ids:\")\n",
        "print(encoded_auto_bert[\"input_ids\"])\n",
        "\n",
        "print(\"\\nTokenized attention_mask:\")\n",
        "print(encoded_auto_bert[\"attention_mask\"])\n",
        "\n",
        "print(\"\\nLongTensor 타입 확인:\")\n",
        "print(type(encoded_auto_bert[\"input_ids\"]))\n",
        "\n",
        "print(\"\\nDecode:\")\n",
        "print(tokenizer_auto_bert.decode(encoded_auto_bert['input_ids'][0]))\n",
        "\n",
        "print(\"\\nTokens:\")\n",
        "print(tokenizer_auto_bert.tokenize(text))\n",
        "\n",
        "print(\"\\ninput_ids:\")\n",
        "print(tokenizer_auto_bert.encode(text))\n",
        "\n",
        "print(\"\\nDecoded input_ids:\")\n",
        "print(tokenizer_auto_bert.decode(encoded_auto_bert[\"input_ids\"][0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zy1F96rT5KAu",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1743521890410,
          "user_tz": -540,
          "elapsed": 12,
          "user": {
            "displayName": "표표표표푱",
            "userId": "15791434277334549064"
          }
        },
        "outputId": "82808196-4e21-4ec7-998c-71d1fa0b6fba"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력된 문장: This movie was incredibly touching and had great performances!! I would definitely recommend it to anyone 🤔🤔.\n",
            "\n",
            "Tokenized input_ids:\n",
            "tensor([[  101,  2023,  3185,  2001, 11757,  7244,  1998,  2018,  2307,  4616,\n",
            "           999,   999,  1045,  2052,  5791, 16755,  2009,  2000,  3087,   100,\n",
            "          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0]])\n",
            "\n",
            "Tokenized attention_mask:\n",
            "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0]])\n",
            "\n",
            "LongTensor 타입 확인:\n",
            "<class 'torch.Tensor'>\n",
            "\n",
            "Decode:\n",
            "[CLS] this movie was incredibly touching and had great performances!! i would definitely recommend it to anyone [UNK]. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "\n",
            "Tokens:\n",
            "['this', 'movie', 'was', 'incredibly', 'touching', 'and', 'had', 'great', 'performances', '!', '!', 'i', 'would', 'definitely', 'recommend', 'it', 'to', 'anyone', '[UNK]', '.']\n",
            "\n",
            "input_ids:\n",
            "[101, 2023, 3185, 2001, 11757, 7244, 1998, 2018, 2307, 4616, 999, 999, 1045, 2052, 5791, 16755, 2009, 2000, 3087, 100, 1012, 102]\n",
            "\n",
            "Decoded input_ids:\n",
            "[CLS] this movie was incredibly touching and had great performances!! i would definitely recommend it to anyone [UNK]. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AutoTokenizer - GPT Tokenizer"
      ],
      "metadata": {
        "id": "oE2F7vvsAeMa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'입력된 문장: {text}')\n",
        "\n",
        "print(\"\\nTokenized input_ids:\")\n",
        "print(encoded_auto_gpt[\"input_ids\"])\n",
        "\n",
        "print(\"\\nTokenized attention_mask:\")\n",
        "print(encoded_auto_gpt[\"attention_mask\"])\n",
        "\n",
        "print(\"\\nLongTensor 타입 확인:\")\n",
        "print(type(encoded_auto_gpt[\"input_ids\"]))\n",
        "\n",
        "print(\"\\nDecode:\")\n",
        "print(tokenizer_auto_gpt.decode(encoded_auto_gpt['input_ids'][0]))\n",
        "\n",
        "print(\"\\nTokens:\")\n",
        "print(tokenizer_auto_gpt.tokenize(text))\n",
        "\n",
        "print(\"\\ninput_ids:\")\n",
        "print(tokenizer_auto_gpt.encode(text))\n",
        "\n",
        "print(\"\\nDecoded input_ids:\")\n",
        "print(tokenizer_auto_gpt.decode(encoded_auto_gpt[\"input_ids\"][0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMeG2JwI9jiT",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1743521947160,
          "user_tz": -540,
          "elapsed": 21,
          "user": {
            "displayName": "표표표표푱",
            "userId": "15791434277334549064"
          }
        },
        "outputId": "756f779f-f2cf-41e4-9077-6193e5a4a364"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력된 문장: This movie was incredibly touching and had great performances!! I would definitely recommend it to anyone 🤔🤔.\n",
            "\n",
            "Tokenized input_ids:\n",
            "tensor([[ 1212,  3807,   373,  8131, 15241,   290,   550,  1049, 13289,  3228,\n",
            "           314,   561,  4753,  4313,   340,   284,  2687, 12520,    97,   242,\n",
            "          8582,    97,   242,    13]])\n",
            "\n",
            "Tokenized attention_mask:\n",
            "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
            "\n",
            "LongTensor 타입 확인:\n",
            "<class 'torch.Tensor'>\n",
            "\n",
            "Decode:\n",
            "This movie was incredibly touching and had great performances!! I would definitely recommend it to anyone 🤔🤔.\n",
            "\n",
            "Tokens:\n",
            "['This', 'Ġmovie', 'Ġwas', 'Ġincredibly', 'Ġtouching', 'Ġand', 'Ġhad', 'Ġgreat', 'Ġperformances', '!!', 'ĠI', 'Ġwould', 'Ġdefinitely', 'Ġrecommend', 'Ġit', 'Ġto', 'Ġanyone', 'ĠðŁ', '¤', 'Ķ', 'ðŁ', '¤', 'Ķ', '.']\n",
            "\n",
            "input_ids:\n",
            "[1212, 3807, 373, 8131, 15241, 290, 550, 1049, 13289, 3228, 314, 561, 4753, 4313, 340, 284, 2687, 12520, 97, 242, 8582, 97, 242, 13]\n",
            "\n",
            "Decoded input_ids:\n",
            "This movie was incredibly touching and had great performances!! I would definitely recommend it to anyone 🤔🤔.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3qmgFKYSAZy7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
