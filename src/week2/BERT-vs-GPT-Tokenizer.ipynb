{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP/qJdHM2Tb9outXOGedF1P"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# BERT vs GPT Tokenizer and AutoTokenizer\n",
        "- ì´ëª¨ì§€ ì²˜ë¦¬ ê´€ì ì—ì„œ ë¹„êµ\n",
        "- ëª¨ë¸ì— ë§ëŠ” tokenizerë¥¼ ìë™ìœ¼ë¡œ ë¶ˆëŸ¬ì˜¤ëŠ” **`AutoTokenizer`**ê¹Œì§€ ì•Œì•„ë³´ì"
      ],
      "metadata": {
        "id": "AQ1luoKUrtCh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BERT Tokenizer"
      ],
      "metadata": {
        "id": "A-9wqD-5yhna"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mHlUdeR4mSNd",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1743517260691,
          "user_tz": -540,
          "elapsed": 6383,
          "user": {
            "displayName": "í‘œí‘œí‘œí‘œí‘±",
            "userId": "15791434277334549064"
          }
        }
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizerFast\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ì´ëª¨ì§€ë¥¼ ë„£ì–´ë³´ì\n",
        "- BERT: ì´ëª¨ì§€ ë°ì´í„° -> [UNK] ì²˜ë¦¬: ë°ì´í„°ì˜ ì™„ì „ ì†ì‹¤ -> ë””ì½”ë”© ì‹œ ì´ëª¨ì§€ ë°ì´í„°ëŠ” ì—†ìŒ\n",
        "- GPT: ì´ëª¨ì§€ ë°ì´í„° -> ì´ëª¨ì§€ë¥¼ ìœ ë‹ˆì½”ë“œ ë¬¸ì ê·¸ëŒ€ë¡œ ì·¨ê¸‰í•˜ì—¬ ì†ì‹¤ ì—†ìŒ / ì´ëª¨ì§€ë¥¼ ë³„ë„ë¡œ ì „ì²˜ë¦¬í•´ì„œ í† í°í™”ë„ ê°€ëŠ¥"
      ],
      "metadata": {
        "id": "eu9GZWkWuuGv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# BERT tokenizer ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# ì˜ˆì œ text\n",
        "text = 'This movie was incredibly touching and had great performances!! I would definitely recommend it to anyone ğŸ¤”ğŸ¤”.'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254,
          "referenced_widgets": [
            "e2906a873ab24767a00ac50f683ff5dd",
            "81717af753f641cb89d4ac8c0355378f",
            "691ab72c73f84b409a3141230f2d2f5b",
            "999c568811a245cb93fd5822ff646c5d",
            "958335bcde7f4ba6ac433e673d13cbcb",
            "409f49ff43c14f248eb40fd029a53b82",
            "bdcde7de8026442c974b7c3e739e3b4f",
            "63ba12936f9843ea835c233022208f3e",
            "2dd33b58eb994863ab63706218adc230",
            "d97df96a25f9444aa8420ddbebacd946",
            "cb8514a40874469da66142b13173448b",
            "cee0233318d940cc8e062a92ba531c54",
            "df6064039a774c05ae31ac9a381738db",
            "b785a44b7e524706afb558de423a06ef",
            "9dd06adb72764091ac0dd7ebbf2dfe2e",
            "9463f7487b15488d8cb1280163a0a9f8",
            "fc6871346af5441c84c9a7fef76b4c85",
            "df0ee36a0db542f6889db9ffadf0ff96",
            "dea0f7187cff4965a40c218b6ca4bfa0",
            "276cd742d1ca401a806a679be0760267",
            "8fdc4ec7b12c4b3c9ab585968ef5a5c3",
            "15e0e39552c449cca984793259202a5e",
            "7d5e07f8eb8f4fe79ed6049a9e6838e2",
            "763957b55d5841c4bd7d405a0a4f300c",
            "7ea939ac83904166bf23a8d35e735e0a",
            "6a503fa1ad6d44d78f9fd138a4cf9cb4",
            "006e20a0f2674c309991554dd899c475",
            "b9c1a5aecfb248c0bccf5da5a0203d8e",
            "1721f0c08f9d40ddaf9c34fc199aecf2",
            "89cdb298050841a5b6f31b513db634fb",
            "66c3d840dfea48e4873236cf01fe1753",
            "e3a450d0374c4822b8770aee20c015f8",
            "b934f24852cb41309e61ba659576f139",
            "cad39f9ad5a84327aac8beffb6c97b92",
            "b5bd3f155c084fabb54b3bddd7ca23a3",
            "894cbada39b14e4b9913ea3b3f8ebc5c",
            "8484cdace7fc44d7989ba5f96d24e93b",
            "de564af199754843a28e94adab18d74e",
            "e89ce3680ecf40bbbf8d0e7e9639065a",
            "3b061ff9cfb8479781205a5afdfcbaea",
            "ecfca5b63b054fba93b3c6d96ad37c48",
            "91546290abc84ba4ac1259fe05ff72d2",
            "1ff77ae845124332874946b164bed246",
            "b0b8dde7371e4872a82c59f34ad0094b"
          ]
        },
        "id": "Wn3Ex6FKntCW",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1743517273614,
          "user_tz": -540,
          "elapsed": 10110,
          "user": {
            "displayName": "í‘œí‘œí‘œí‘œí‘±",
            "userId": "15791434277334549064"
          }
        },
        "outputId": "e656348f-026e-49cc-8000-b7e5f7c0f8da"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e2906a873ab24767a00ac50f683ff5dd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cee0233318d940cc8e062a92ba531c54"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7d5e07f8eb8f4fe79ed6049a9e6838e2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cad39f9ad5a84327aac8beffb6c97b92"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer ë™ì‘ í™•ì¸\n",
        "encoded = tokenizer(\n",
        "    text,\n",
        "    padding='max_length', # padding\n",
        "    truncation=True,      # ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸°\n",
        "    max_length=30,        # ìµœëŒ€ ê¸¸ì´ 20 ì œí•œ\n",
        "    return_tensors='pt'   # PyTorch í…ì„œë¡œ ë³€í™˜\n",
        ")\n",
        "\n",
        "tokens_bert = tokenizer.tokenize(text)\n",
        "input_ids_bert = tokenizer.encode(text)\n",
        "decoded_bert = tokenizer.decode(input_ids_bert)"
      ],
      "metadata": {
        "id": "Ih7-uvTsoJMd",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1743518813353,
          "user_tz": -540,
          "elapsed": 2808,
          "user": {
            "displayName": "í‘œí‘œí‘œí‘œí‘±",
            "userId": "15791434277334549064"
          }
        }
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERT Tokenizer ì¶œë ¥ ê²°ê³¼"
      ],
      "metadata": {
        "id": "UMIDwuTF3REW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'ì…ë ¥ëœ ë¬¸ì¥: {text}')\n",
        "\n",
        "print(\"\\nTokenized input_ids:\")\n",
        "print(encoded[\"input_ids\"])\n",
        "\n",
        "print(\"\\nTokenized attention_mask:\")\n",
        "print(encoded[\"attention_mask\"])\n",
        "\n",
        "print(\"\\nLongTensor íƒ€ì… í™•ì¸:\")\n",
        "print(type(encoded[\"input_ids\"]))\n",
        "\n",
        "print(\"\\nDecode:\")\n",
        "print(tokenizer.decode(encoded['input_ids'][0]))\n",
        "\n",
        "print(\"\\nTokens:\")\n",
        "print(tokens_bert)\n",
        "\n",
        "print(\"\\ninput_ids:\")\n",
        "print(input_ids_bert)\n",
        "\n",
        "print(\"\\nDecoded input_ids:\")\n",
        "print(decoded_bert)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3j3igOBocGe",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1743518882995,
          "user_tz": -540,
          "elapsed": 15,
          "user": {
            "displayName": "í‘œí‘œí‘œí‘œí‘±",
            "userId": "15791434277334549064"
          }
        },
        "outputId": "d0079b49-559c-4fce-f6dd-cc0e59d1882f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì…ë ¥ëœ ë¬¸ì¥: This movie was incredibly touching and had great performances!! I would definitely recommend it to anyone ğŸ¤”ğŸ¤”.\n",
            "\n",
            "Tokenized input_ids:\n",
            "tensor([[  101,  2023,  3185,  2001, 11757,  7244,  1998,  2018,  2307,  4616,\n",
            "           999,   999,  1045,  2052,  5791, 16755,  2009,  2000,  3087,   100,\n",
            "          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0]])\n",
            "\n",
            "Tokenized attention_mask:\n",
            "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0]])\n",
            "\n",
            "LongTensor íƒ€ì… í™•ì¸:\n",
            "<class 'torch.Tensor'>\n",
            "\n",
            "Decode:\n",
            "[CLS] this movie was incredibly touching and had great performances!! i would definitely recommend it to anyone [UNK]. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "\n",
            "Tokens:\n",
            "['this', 'movie', 'was', 'incredibly', 'touching', 'and', 'had', 'great', 'performances', '!', '!', 'i', 'would', 'definitely', 'recommend', 'it', 'to', 'anyone', '[UNK]', '.']\n",
            "\n",
            "input_ids:\n",
            "[101, 2023, 3185, 2001, 11757, 7244, 1998, 2018, 2307, 4616, 999, 999, 1045, 2052, 5791, 16755, 2009, 2000, 3087, 100, 1012, 102]\n",
            "\n",
            "Decoded input_ids:\n",
            "[CLS] this movie was incredibly touching and had great performances!! i would definitely recommend it to anyone [UNK]. [SEP]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "|í•­ëª©|ì„¤ëª…|\n",
        "|:---|:----------------------------|\n",
        "|`input_ids`|\tí† í¬ë‚˜ì´ì§•ëœ BERTìš© ìˆ«ì ì‹œí€€ìŠ¤|\n",
        "|`attention_mask`|\tì‹¤ì œ ë‹¨ì–´ëŠ” 1, paddingì€ 0|\n",
        "|ê¸¸ì´|\t`max_length=20`ìœ¼ë¡œ ì œí•œë˜ì—ˆê¸° ë•Œë¬¸ì— 20ê°œ|\n",
        "|`type`|\t`<class 'torch.Tensor'>` â†’ LongTensorë¡œ ìë™ ë³€í™˜|"
      ],
      "metadata": {
        "id": "KFtQRyQpo7rP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenized `input_ids`\n",
        "```\n",
        "tensor([\n",
        "  [101, 2023, 3185, 2001, 11757, 7244, 1998, 2018, 2307, 4616,\n",
        "   999,  999, 1045, 2052, 5791, 16755, 2009, 2000, 3087, 100,\n",
        "   1012, 102, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "])\n",
        "```\n",
        "\n",
        "|í† í° ID|\tì˜ë¯¸|\n",
        "|:---|:---|\n",
        "|101|\t[CLS] â†’ ë¬¸ì¥ì˜ ì‹œì‘|\n",
        "|100|\t[UNK] â†’ ğŸ¤” ì´ëª¨ì§€ë¥¼ í† í¬ë‚˜ì´ì €ê°€ ëª¨ë¦„ â†’ \"Unknown\" í† í°ìœ¼ë¡œ ëŒ€ì²´ ğŸ‘‰ğŸ» ì´ëª¨ì§€ 2ê°œë¥¼ í•˜ë‚˜ë¡œ í‰ì¹¨|\n",
        "|1012|\t. ë§ˆì¹¨í‘œ|\n",
        "|102|\t[SEP] â†’ ë¬¸ì¥ì˜ ë|\n",
        "|0|\t[PAD] â†’ max_length 30ì— ë§ì¶° ë¹ˆìë¦¬ë¥¼ ì±„ì›€|\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "### Tokenized `attention_mask`\n",
        "```\n",
        "tensor([\n",
        "  [1, 1, 1, ..., 1, 1, 0, 0, 0, ..., 0]\n",
        "])\n",
        "```\n",
        "- 1: ì‹¤ì œ ë‹¨ì–´ ë˜ëŠ” íŠ¹ìˆ˜ í† í° ([CLS], [SEP])\n",
        "- 0: padding ([PAD] ìë¦¬)   \n",
        "  â†’ ëª¨ë¸ì´ í•™ìŠµí•  ë•Œ **0ì¸ ìœ„ì¹˜ëŠ” ë¬´ì‹œ**\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "### Decoding Result\n",
        "```\n",
        "[CLS] this movie was incredibly touching and had great performances!! i would definitely recommend it to anyone [UNK]. [SEP] [PAD] [PAD] ...\n",
        "```\n",
        "- `tokenizer.decode(input_ids)`: tokení™” ëœ input tensorë¥¼ ë‹¤ì‹œ NLë¡œ ë³€í™˜"
      ],
      "metadata": {
        "id": "N9BpcOkCu2K1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPT Tokenizer"
      ],
      "metadata": {
        "id": "JMSQESw4ynap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2TokenizerFast"
      ],
      "metadata": {
        "id": "9pwJ26weyovh",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1743518364220,
          "user_tz": -540,
          "elapsed": 52,
          "user": {
            "displayName": "í‘œí‘œí‘œí‘œí‘±",
            "userId": "15791434277334549064"
          }
        }
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ì‹¤ìŠµ textëŠ” ìœ„ì— ì„ ì–¸í•œ text ê·¸ëŒ€ë¡œ ì‚¬ìš©\n",
        "- ì •í™•í•œ ë¹„êµë¥¼ ìœ„í•¨"
      ],
      "metadata": {
        "id": "emy2ZJUEy8wb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GTP2 í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "tokenizer_gpt = GPT2TokenizerFast.from_pretrained(\"gpt2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177,
          "referenced_widgets": [
            "4c7dc6e79d6544b68091d431ca98db2a",
            "0827a7b53f3b4cc4bfffc2ebd01bf2d4",
            "7cc2417e89f946f28de476cb7b41ba77",
            "4cf59125f02448cf995170a455060a17",
            "a1e223cdb9eb49bb9587ef744d50d944",
            "c5e22180453249eb996d0a50399ec5fc",
            "191fea53e1fd4c498e8189d0eb8a43bc",
            "7a873e39a7cd42bbbe9e67eb9df7706b",
            "ef045c3c761d417ebf36153143b3a09c",
            "21cc793b3a5e4b2a9694bb59afd0a188",
            "389865c2031d47829431c46265287a7b",
            "c5bb0c64011445dea07b25ed9d3b9256",
            "9074db6133a24951a9854462a372074d",
            "504d8d239a0c4c75af7fe4bf6e4add1e",
            "448fd463c57148eb8f22d0fdca90767d",
            "accd9301467543909449477f4a0c955c",
            "8f18472538a44954a3d9de5f21f5176c",
            "3b5beffb02824b23948ae02ee34ca269",
            "9dd676b7ba7c401ba5567db423b584d5",
            "8b6ef9946d2a4f0da11d30c86bea0f64",
            "78ff8e1854284522895dd9c3ce440c49",
            "c7d31ae1f33445debbd78975d77b5ced",
            "ff465280c873440e835d4aa1fc01dc69",
            "92077e60fb1e4036a47a2b1136b1c809",
            "1eeed87f9ad7437da193b8d1e59fdc63",
            "2b08112402eb4dfe88328bcaba8b663a",
            "d1c01fb236674938a5b77418c1d56228",
            "b716bc71f4c94384bf55887a0d154ca1",
            "1afd6ddbcae34d8cb35460f8c255fd9d",
            "131e319fcf47411594da5873cf0c0b23",
            "31824b6d7fc54ad69e4649c68b395c08",
            "ab9000c086394ebc898a186c0c6cac36",
            "393e7738376242ddbc932b6b5cd73a49",
            "87c727683dcb48fdb766677c174bc161",
            "8fc8332b3b40488fb1fa28321e62c9e1",
            "b1401d5cd9b546418d5a93bb220f6ad0",
            "8b884ccea28c494dbee34fd53b5bd7ef",
            "f83545537d514e0facd29c00675d86ce",
            "39b327380e1f456ab8323082dbc5296e",
            "1704527bc7d9437ca64b7ca8cd5d8d89",
            "f961a7122174413b9c2eb69a1f80cadd",
            "31dccaec27764704a0a2ca593e1f120b",
            "af70c4e6ca2a48ffacb768caf58b3521",
            "77793944a9dc4f34a8b5bde85df3dcc2",
            "1c2eb95d63a4419683241c6777ddabbf",
            "4bca93bd2db041c7843bae7b9c8559b4",
            "91f2e649c0e845ea879f9ea80ec95140",
            "c2a39e59ffe34803b6884b888c099257",
            "caffb7e694bb4a81a3317b927f2f7bb2",
            "8b8e41c3c6cc43a29b6a83c544b75dfb",
            "1635b81fde6c48d6aa8702a69ff2ad89",
            "9ba0221090cd4517844f093c8f86efb9",
            "a91c302daa3e4efeb58856c13f0202d3",
            "5e5b13b809504f97a9923e22761910d8",
            "4d6afe37482a47f8aa63a4de14cf6834"
          ]
        },
        "id": "khIkKWc_yvCQ",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1743518419385,
          "user_tz": -540,
          "elapsed": 2669,
          "user": {
            "displayName": "í‘œí‘œí‘œí‘œí‘±",
            "userId": "15791434277334549064"
          }
        },
        "outputId": "993599ca-9b09-4078-c82b-6eb2b11ee36a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4c7dc6e79d6544b68091d431ca98db2a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c5bb0c64011445dea07b25ed9d3b9256"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ff465280c873440e835d4aa1fc01dc69"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "87c727683dcb48fdb766677c174bc161"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1c2eb95d63a4419683241c6777ddabbf"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GPTëŠ” paddingì„ ì§€ì›í•˜ì§€ ì•ŠëŠ”ë‹¤**   \n",
        "- GPTëŠ” **ë¬¸ì¥ ìƒì„±ìš© ëª¨ë¸**ì´ê¸° ë•Œë¬¸ì— í•œ ë¬¸ì¥ì”© **ì˜¤ë¥¸ìª½ìœ¼ë¡œë§Œ ê¸¸ì–´ì§€ëŠ”** êµ¬ì¡°\n",
        "  ```\n",
        "  GPTëŠ” ì›ë˜ paddingì´ í•„ìš”í•˜ì§€ ì•Šë‹¤\n",
        "  ```\n",
        "- BERTëŠ” ë¬¸ì¥ ë¶„ë¥˜ìš©ìœ¼ë¡œ **íŒ¨ë”© + ë§ˆìŠ¤í‚¹**ì´ ì¼ë°˜ì ì´ë‹¤\n",
        "- GPT2 Tokenì— pad_tokenì„ ëª…ì‹œì ìœ¼ë¡œ ì¶”ê°€í•˜ë©´ padding ì˜µì…˜ë„ ì •ìƒ ì‘ë™í•˜ê¸´ í•˜ëŠ”ë° êµ³ì´ ëŠë‚Œì„\n",
        "  ``` python\n",
        "  # ì´ëŸ° ì‹ìœ¼ë¡œ í•˜ë©´ ë¨\n",
        "  tokenizer_gpt.pad_token = tokenizer_gpt.eos_token\n",
        "  ```\n",
        "\n",
        "  <br>\n",
        "\n",
        "**Summary**   \n",
        "\n",
        "|ì›ì¸|í•´ê²° ë°©ë²•|\n",
        "|:---|:---|\n",
        "|GPT tokenizerëŠ” ê¸°ë³¸ì ìœ¼ë¡œ `pad_token` ì—†ìŒ|\t`tokenizer.pad_token = tokenizer.eos_token` ìœ¼ë¡œ ì§€ì •|\n",
        "|padding='max_length' ì‚¬ìš© ì‹œ ì—ëŸ¬ ë°œìƒ ê°€ëŠ¥|ìœ„ì²˜ëŸ¼ `pad_token`ì„ ì„¤ì •í•˜ë©´ í•´ê²°|\n",
        "|BERTì™€ GPTëŠ” í† í¬ë‚˜ì´ì € ê¸°ë³¸ ì„¤ì •ì´ ë‹¤ë¦„|`AutoTokenizer`ë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜ ì»¤ìŠ¤í„°ë§ˆì´ì¦ˆ í•„ìš”|"
      ],
      "metadata": {
        "id": "RHppsyYh1-s7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_gpt = tokenizer_gpt(\n",
        "    text,\n",
        "    # padding='max_length', # padding\n",
        "    truncation=True,      # ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸°\n",
        "    max_length=30,        # ìµœëŒ€ ê¸¸ì´ 20 ì œí•œ\n",
        "    return_tensors='pt'   # PyTorch í…ì„œë¡œ ë³€í™˜\n",
        ")"
      ],
      "metadata": {
        "id": "CNIF8-FUy74Q",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1743519008279,
          "user_tz": -540,
          "elapsed": 2,
          "user": {
            "displayName": "í‘œí‘œí‘œí‘œí‘±",
            "userId": "15791434277334549064"
          }
        }
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_gpt = tokenizer_gpt.tokenize(text)\n",
        "input_ids_gpt = tokenizer_gpt.encode(text)\n",
        "decoded_gpt = tokenizer_gpt.decode(input_ids_gpt)"
      ],
      "metadata": {
        "id": "lw8mirVrzV6I",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1743519176775,
          "user_tz": -540,
          "elapsed": 5,
          "user": {
            "displayName": "í‘œí‘œí‘œí‘œí‘±",
            "userId": "15791434277334549064"
          }
        }
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPT Tokenizer ì¶œë ¥ ê²°ê³¼"
      ],
      "metadata": {
        "id": "iRPwsIfo3VaJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'ì…ë ¥ëœ ë¬¸ì¥: {text}')\n",
        "\n",
        "print(\"\\nTokenized input_ids:\")\n",
        "print(encoded_gpt[\"input_ids\"])\n",
        "\n",
        "print(\"\\nTokenized attention_mask:\")\n",
        "print(encoded_gpt[\"attention_mask\"])\n",
        "\n",
        "print(\"\\nLongTensor íƒ€ì… í™•ì¸:\")\n",
        "print(type(encoded_gpt[\"input_ids\"]))\n",
        "\n",
        "print(\"\\nDecode:\")\n",
        "print(tokenizer_gpt.decode(encoded_gpt['input_ids'][0]))\n",
        "\n",
        "print(\"\\nTokens:\")\n",
        "print(tokens_gpt)\n",
        "\n",
        "print(\"\\ninput_ids:\")\n",
        "print(input_ids_gpt)\n",
        "\n",
        "print(\"\\nDecoded input_ids:\")\n",
        "print(decoded_gpt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2YtsC7V1UCI",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1743519190087,
          "user_tz": -540,
          "elapsed": 52,
          "user": {
            "displayName": "í‘œí‘œí‘œí‘œí‘±",
            "userId": "15791434277334549064"
          }
        },
        "outputId": "dd4fb531-5730-48e1-8180-8bd2bd2d6dd9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì…ë ¥ëœ ë¬¸ì¥: This movie was incredibly touching and had great performances!! I would definitely recommend it to anyone ğŸ¤”ğŸ¤”.\n",
            "\n",
            "Tokenized input_ids:\n",
            "tensor([[ 1212,  3807,   373,  8131, 15241,   290,   550,  1049, 13289,  3228,\n",
            "           314,   561,  4753,  4313,   340,   284,  2687, 12520,    97,   242,\n",
            "          8582,    97,   242,    13]])\n",
            "\n",
            "Tokenized attention_mask:\n",
            "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
            "\n",
            "LongTensor íƒ€ì… í™•ì¸:\n",
            "<class 'torch.Tensor'>\n",
            "\n",
            "Decode:\n",
            "This movie was incredibly touching and had great performances!! I would definitely recommend it to anyone ğŸ¤”ğŸ¤”.\n",
            "\n",
            "Tokens:\n",
            "['This', 'Ä movie', 'Ä was', 'Ä incredibly', 'Ä touching', 'Ä and', 'Ä had', 'Ä great', 'Ä performances', '!!', 'Ä I', 'Ä would', 'Ä definitely', 'Ä recommend', 'Ä it', 'Ä to', 'Ä anyone', 'Ä Ã°Å', 'Â¤', 'Ä¶', 'Ã°Å', 'Â¤', 'Ä¶', '.']\n",
            "\n",
            "input_ids:\n",
            "[1212, 3807, 373, 8131, 15241, 290, 550, 1049, 13289, 3228, 314, 561, 4753, 4313, 340, 284, 2687, 12520, 97, 242, 8582, 97, 242, 13]\n",
            "\n",
            "Decoded input_ids:\n",
            "This movie was incredibly touching and had great performances!! I would definitely recommend it to anyone ğŸ¤”ğŸ¤”.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AutoTokenizer"
      ],
      "metadata": {
        "id": "WYl4P59Q4lLj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "V-d1xflx4pNL",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1743519941561,
          "user_tz": -540,
          "elapsed": 5,
          "user": {
            "displayName": "í‘œí‘œí‘œí‘œí‘±",
            "userId": "15791434277334549064"
          }
        }
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AutoTokenizerì˜ íš¨ê³¼\n",
        "- ì½”ë“œ ì¬ì‚¬ìš©ì„±ì´ ë†’ì•„ì§„ë‹¤\n",
        "  ``` python\n",
        "  from transformers import BertTokenizerFast\n",
        "  from transformers import GPT2TokenizerFast\n",
        "  ```\n",
        "  ê¸°ì¡´ì—ëŠ” ìœ„ ì½”ë“œì²˜ëŸ¼ tokenizerë¥¼ ê°ê° import\n",
        "\n",
        "- í•˜ë‹¨ê³¼ ê°™ì´ ì½”ë“œë¥¼ `AutoTokenizer`ë¡œ ë¬¶ê²Œë˜ë©´, ê·¸ì— ìƒì‘í•˜ëŠ” tokenizerë¥¼ ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤\n",
        "  - **importë¥¼ AutoTokenizer í•˜ë‚˜ë§Œ í•˜ë©´ ë˜ê²Œ ë˜ëŠ” ê²ƒ**\n",
        "\n",
        "### í•œê³„ì \n",
        "- AutoTokenizerëŠ” ì½”ë“œ ì¬ì‚¬ìš©ì„±ì„ ìœ„í•¨ì¼ ë¿\n",
        "- í…ŒìŠ¤íŠ¸ í™˜ê²½ì—ì„œ ì‚¬ìš©ì„ ì§€í–¥í•˜ê³ , í”„ë¡œë•íŠ¸ ë°°í¬ ì‹œì—ëŠ” ì •í™•í•œ Tokenizerë¥¼ importí•´ì„œ ì‚¬ìš©í•˜ëŠ” ê²Œ ì„±ëŠ¥/ì•ˆì •ì„± ë©´ì—ì„œ ì¢‹"
      ],
      "metadata": {
        "id": "Ts_ZxQuO6Bde"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_auto_gpt = AutoTokenizer.from_pretrained('gpt2')\n",
        "tokenizer_auto_bert = AutoTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "ZLV-oMXV4wJ0",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1743519985058,
          "user_tz": -540,
          "elapsed": 437,
          "user": {
            "displayName": "í‘œí‘œí‘œí‘œí‘±",
            "userId": "15791434277334549064"
          }
        }
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_auto_gpt = tokenizer_auto_gpt(\n",
        "    text,\n",
        "    # padding='max_length', # GPTëŠ” padding ì§€ì›í•˜ì§€ ì•ŠëŠ”ë‹¤\n",
        "    truncation=True,      # ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸°\n",
        "    max_length=30,        # ìµœëŒ€ ê¸¸ì´ 20 ì œí•œ\n",
        "    return_tensors='pt'   # PyTorch í…ì„œë¡œ ë³€í™˜\n",
        ")\n",
        "\n",
        "encoded_auto_bert = tokenizer_auto_bert(\n",
        "    text,\n",
        "    padding='max_length', # padding\n",
        "    truncation=True,      # ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸°\n",
        "    max_length=30,        # ìµœëŒ€ ê¸¸ì´ 20 ì œí•œ\n",
        "    return_tensors='pt'   # PyTorch í…ì„œë¡œ ë³€í™˜\n",
        ")"
      ],
      "metadata": {
        "id": "uXgHvvPN46qi",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1743521924628,
          "user_tz": -540,
          "elapsed": 14,
          "user": {
            "displayName": "í‘œí‘œí‘œí‘œí‘±",
            "userId": "15791434277334549064"
          }
        }
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AutoTokenize - BERT Tokenizer"
      ],
      "metadata": {
        "id": "0K55ArfEAO5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'ì…ë ¥ëœ ë¬¸ì¥: {text}')\n",
        "\n",
        "print(\"\\nTokenized input_ids:\")\n",
        "print(encoded_auto_bert[\"input_ids\"])\n",
        "\n",
        "print(\"\\nTokenized attention_mask:\")\n",
        "print(encoded_auto_bert[\"attention_mask\"])\n",
        "\n",
        "print(\"\\nLongTensor íƒ€ì… í™•ì¸:\")\n",
        "print(type(encoded_auto_bert[\"input_ids\"]))\n",
        "\n",
        "print(\"\\nDecode:\")\n",
        "print(tokenizer_auto_bert.decode(encoded_auto_bert['input_ids'][0]))\n",
        "\n",
        "print(\"\\nTokens:\")\n",
        "print(tokenizer_auto_bert.tokenize(text))\n",
        "\n",
        "print(\"\\ninput_ids:\")\n",
        "print(tokenizer_auto_bert.encode(text))\n",
        "\n",
        "print(\"\\nDecoded input_ids:\")\n",
        "print(tokenizer_auto_bert.decode(encoded_auto_bert[\"input_ids\"][0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zy1F96rT5KAu",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1743521890410,
          "user_tz": -540,
          "elapsed": 12,
          "user": {
            "displayName": "í‘œí‘œí‘œí‘œí‘±",
            "userId": "15791434277334549064"
          }
        },
        "outputId": "82808196-4e21-4ec7-998c-71d1fa0b6fba"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì…ë ¥ëœ ë¬¸ì¥: This movie was incredibly touching and had great performances!! I would definitely recommend it to anyone ğŸ¤”ğŸ¤”.\n",
            "\n",
            "Tokenized input_ids:\n",
            "tensor([[  101,  2023,  3185,  2001, 11757,  7244,  1998,  2018,  2307,  4616,\n",
            "           999,   999,  1045,  2052,  5791, 16755,  2009,  2000,  3087,   100,\n",
            "          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0]])\n",
            "\n",
            "Tokenized attention_mask:\n",
            "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0]])\n",
            "\n",
            "LongTensor íƒ€ì… í™•ì¸:\n",
            "<class 'torch.Tensor'>\n",
            "\n",
            "Decode:\n",
            "[CLS] this movie was incredibly touching and had great performances!! i would definitely recommend it to anyone [UNK]. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "\n",
            "Tokens:\n",
            "['this', 'movie', 'was', 'incredibly', 'touching', 'and', 'had', 'great', 'performances', '!', '!', 'i', 'would', 'definitely', 'recommend', 'it', 'to', 'anyone', '[UNK]', '.']\n",
            "\n",
            "input_ids:\n",
            "[101, 2023, 3185, 2001, 11757, 7244, 1998, 2018, 2307, 4616, 999, 999, 1045, 2052, 5791, 16755, 2009, 2000, 3087, 100, 1012, 102]\n",
            "\n",
            "Decoded input_ids:\n",
            "[CLS] this movie was incredibly touching and had great performances!! i would definitely recommend it to anyone [UNK]. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AutoTokenizer - GPT Tokenizer"
      ],
      "metadata": {
        "id": "oE2F7vvsAeMa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'ì…ë ¥ëœ ë¬¸ì¥: {text}')\n",
        "\n",
        "print(\"\\nTokenized input_ids:\")\n",
        "print(encoded_auto_gpt[\"input_ids\"])\n",
        "\n",
        "print(\"\\nTokenized attention_mask:\")\n",
        "print(encoded_auto_gpt[\"attention_mask\"])\n",
        "\n",
        "print(\"\\nLongTensor íƒ€ì… í™•ì¸:\")\n",
        "print(type(encoded_auto_gpt[\"input_ids\"]))\n",
        "\n",
        "print(\"\\nDecode:\")\n",
        "print(tokenizer_auto_gpt.decode(encoded_auto_gpt['input_ids'][0]))\n",
        "\n",
        "print(\"\\nTokens:\")\n",
        "print(tokenizer_auto_gpt.tokenize(text))\n",
        "\n",
        "print(\"\\ninput_ids:\")\n",
        "print(tokenizer_auto_gpt.encode(text))\n",
        "\n",
        "print(\"\\nDecoded input_ids:\")\n",
        "print(tokenizer_auto_gpt.decode(encoded_auto_gpt[\"input_ids\"][0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMeG2JwI9jiT",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1743521947160,
          "user_tz": -540,
          "elapsed": 21,
          "user": {
            "displayName": "í‘œí‘œí‘œí‘œí‘±",
            "userId": "15791434277334549064"
          }
        },
        "outputId": "756f779f-f2cf-41e4-9077-6193e5a4a364"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì…ë ¥ëœ ë¬¸ì¥: This movie was incredibly touching and had great performances!! I would definitely recommend it to anyone ğŸ¤”ğŸ¤”.\n",
            "\n",
            "Tokenized input_ids:\n",
            "tensor([[ 1212,  3807,   373,  8131, 15241,   290,   550,  1049, 13289,  3228,\n",
            "           314,   561,  4753,  4313,   340,   284,  2687, 12520,    97,   242,\n",
            "          8582,    97,   242,    13]])\n",
            "\n",
            "Tokenized attention_mask:\n",
            "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
            "\n",
            "LongTensor íƒ€ì… í™•ì¸:\n",
            "<class 'torch.Tensor'>\n",
            "\n",
            "Decode:\n",
            "This movie was incredibly touching and had great performances!! I would definitely recommend it to anyone ğŸ¤”ğŸ¤”.\n",
            "\n",
            "Tokens:\n",
            "['This', 'Ä movie', 'Ä was', 'Ä incredibly', 'Ä touching', 'Ä and', 'Ä had', 'Ä great', 'Ä performances', '!!', 'Ä I', 'Ä would', 'Ä definitely', 'Ä recommend', 'Ä it', 'Ä to', 'Ä anyone', 'Ä Ã°Å', 'Â¤', 'Ä¶', 'Ã°Å', 'Â¤', 'Ä¶', '.']\n",
            "\n",
            "input_ids:\n",
            "[1212, 3807, 373, 8131, 15241, 290, 550, 1049, 13289, 3228, 314, 561, 4753, 4313, 340, 284, 2687, 12520, 97, 242, 8582, 97, 242, 13]\n",
            "\n",
            "Decoded input_ids:\n",
            "This movie was incredibly touching and had great performances!! I would definitely recommend it to anyone ğŸ¤”ğŸ¤”.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3qmgFKYSAZy7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
