{"cells":[{"cell_type":"markdown","metadata":{"id":"ymxatB5WYxlL"},"source":["# Transformer - 기본 과제\n","> 주어진 문장 다음 단어를 예측하는 모델을 만들어본다\n","   \n","\n","\n","### RNN vs Transformer\n","| 항목             | RNN (Recurrent Neural Network)                             | Transformer                                               |\n","|:------------------|-------------------------------------------------------------|------------------------------------------------------------|\n","| **기본 구조**     | 순차적으로 시퀀스를 처리 (시간 순서대로)                  | 전체 시퀀스를 동시에 처리 (병렬처리 가능)                  |\n","| **병렬 처리**     | ❌ 불가능 (이전 단계 결과가 다음 단계 입력에 필요)         | ✅ 가능 (Self-Attention으로 모든 위치를 동시에 참조)       |\n","| **장기 의존성 처리** | ❌ 어려움 (Vanishing Gradient 문제)                      | ✅ 훨씬 강력함 (모든 단어 간 관계 파악 가능)               |\n","| **입력 위치 정보** | 자연스럽게 순서를 따름                                    | 별도로 Positional Encoding 필요                           |\n","| **대표 모델**     | LSTM, GRU                                                  | BERT, GPT, T5, etc.                                       |\n","| **성능 및 속도**  | 긴 문장 처리에 약함 / 느림                                 | 긴 문장도 잘 처리 / 빠름 (GPU 병렬화)                      |\n","\n","\n","   \n","### IMDb 감성 분류 데이터셋\n","- Internet Movie Database\n","- 감성 분류(Sentiment Classification)\n","  - 영화에 대한 정보 / 사용자 리뷰(text) / 별점\n","  - 입력: 사용자 리뷰(text)\n","  - 출력: 감성 라벨(긍정 or 부정)\n","- 텍스트 분류(Binary Classification) 문제\n","\n","**Example**\n","``` makefile\n","입력: \"This movie was surprisingly good and well-acted.\"\n","출력: 1 (긍정)\n","```\n","\n","``` makefile\n","입력: \"I wasted two hours of my life watching this.\"\n","출력: 0 (부정)\n","```\n","\n","### import 목록\n","- `datasets`: Hugging Face의 데이터셋 로더 라이브러리 / IMDb, MNIST 등 다양한 데이터셋 활용 가능\n","- `sacremoses`: 텍스트 전처리 모듈 / tokenizer 내부에서 사용할 수 있음\n"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"1X7RM2du1zcr","outputId":"9b5b693f-8bb3-4f4d-9d7b-adfb3a0a4598","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1743529946959,"user_tz":-540,"elapsed":4337,"user":{"displayName":"표표표표푱","userId":"15791434277334549064"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting datasets\n","  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n","Collecting sacremoses\n","  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess<0.70.17 (from datasets)\n","  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n","Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n","  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.14)\n","Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.29.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacremoses) (2024.11.6)\n","Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from sacremoses) (8.1.8)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from sacremoses) (1.4.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n","Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: xxhash, sacremoses, fsspec, dill, multiprocess, datasets\n","  Attempting uninstall: fsspec\n","    Found existing installation: fsspec 2025.3.0\n","    Uninstalling fsspec-2025.3.0:\n","      Successfully uninstalled fsspec-2025.3.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.12.0 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 sacremoses-0.1.1 xxhash-3.5.0\n"]}],"source":["!pip install datasets sacremoses"]},{"cell_type":"markdown","source":["# **Attention is all you need**   \n","![Attention is all you need](https://binarymindset.com/wp-content/uploads/2023/08/transformers-4-574x1024.png)   \n","- 6 encoder - 6 decoder structure\n","\n","## Transformer 구조\n","\n","### 1️⃣Embedding   \n","### 2️⃣Positional Encoding   \n","### 3️⃣Encoder   \n","- Self-Attention\n","- Feed Forward\n","\n","### 4️⃣Decoder  \n","- Masked Self-Attention\n","- Encoder-Decoder Attention\n","- Feed Forward\n","\n","### 5️⃣Prediction   \n","\n"],"metadata":{"id":"aOck_fe_E120"}},{"cell_type":"markdown","source":["### Load Tokenizer from Hugging Face\n","\n","**BERT vs GPT2 Tokenizer | AutoTokenizer** 정리  \n","[![Notebook](https://img.shields.io/badge/Jupyter-Notebook-orange?logo=jupyter)](https://nbviewer.org/github/zerovodka/ML-learning/blob/master/src/week2/BERT-vs-GPT-Tokenizer.ipynb?flush_cache=true)\n","- Emoji 처리 관점에서 비교\n","- AutoTokenizer\n","- 실무 vs 실험 적절한 import\n","- tokenizer 내부 성분 값 설명\n"],"metadata":{"id":"Ybp8TvFAZacg"}},{"cell_type":"code","execution_count":3,"metadata":{"id":"HOdhoBVA1zcu","executionInfo":{"status":"ok","timestamp":1743529998805,"user_tz":-540,"elapsed":5,"user":{"displayName":"표표표표푱","userId":"15791434277334549064"}}},"outputs":[],"source":["import torch\n","from datasets import load_dataset\n","from torch.utils.data import DataLoader\n","from transformers import BertTokenizerFast\n","from tokenizers import (\n","    decoders,\n","    models,\n","    normalizers,\n","    pre_tokenizers,\n","    processors,\n","    trainers,\n","    Tokenizer,\n",")"]},{"cell_type":"markdown","source":["**Load BERT Tokenizer**   \n","- BERT Tokenizer Model의 사전 학습된 구조, 설정, vocabulary를 확인할 수 있다"],"metadata":{"id":"7eAIcMnCfyUI"}},{"cell_type":"code","source":["# ds = load_dataset(\"stanfordnlp/imdb\")\n","\n","# imdb 데이터 앞에서 5%만 잘라서 수행: 빠른 실험을 위함\n","train_ds = load_dataset(\"stanfordnlp/imdb\", split=\"train[:5%]\")\n","test_ds = load_dataset(\"stanfordnlp/imdb\", split=\"test[:5%]\")\n","\n","tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n","\n","print(tokenizer)\n","\n","print(f'사용중인 tokenizer 모델: \\n{tokenizer.name_or_path}')\n","print(f'\\n\\n{tokenizer.name_or_path} 모델의 vacabulary size: \\n{tokenizer.vocab_size}')\n","print(f'\\n\\n{tokenizer.name_or_path} 모델의 max length: \\n{tokenizer.model_max_length}')\n","print(f'\\n\\n{tokenizer.name_or_path} 모델의 special_tokens_map: \\n{tokenizer.special_tokens_map}')\n","print(f'\\n\\n{tokenizer.name_or_path} 모델의 decode 시 특수 token 처리 방식: \\n{tokenizer.added_tokens_decoder}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ACgKfXLSfIXR","executionInfo":{"status":"ok","timestamp":1743531525585,"user_tz":-540,"elapsed":2414,"user":{"displayName":"표표표표푱","userId":"15791434277334549064"}},"outputId":"b598cd23-59c5-42c4-a847-ab701c41a286"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n","\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}\n",")\n","사용중인 tokenizer 모델: \n","bert-base-uncased\n","\n","\n","bert-base-uncased 모델의 vacabulary size: \n","30522\n","\n","\n","bert-base-uncased 모델의 max length: \n","512\n","\n","\n","bert-base-uncased 모델의 special_tokens_map: \n","{'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}\n","\n","\n","bert-base-uncased 모델의 decode 시 특수 token 처리 방식: \n","{0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True), 100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True), 101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True), 102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True), 103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True)}\n"]}]},{"cell_type":"markdown","source":["**Encode function**\n","- tokenizer 모델 마다 설정 값이 다르기때문에, 각각에 맞는 encode 함수를 선언한다\n","\n","**기능**\n","- tokenizing: `text` -> token 쪼개기\n","- encoding: token -> number ID 인코딩\n","- padding / truncation: 패딩, 길이 잘라내기(max length 선언 기준)\n","- return: 딕셔너리 리턴\n","  - `return_tensor='pt'` 속성 추가 시: python tensor 리턴   \n","\n","|옵션|설명|\n","|:---|:---|\n","|**`return_tensors='pt'`**|\tPyTorch 텐서로 리턴 → 모델 입력에 바로 사용 가능|\n","|**`return_tensors='tf'`**|\tTensorFlow용|\n","|**`return_tensors='np'`**|\tNumPy용\n","|**`return_tensors=None`**|\t파이썬 dict (list 형태) → 디버깅/출력 확인에 유리"],"metadata":{"id":"63tvIaxFlXU2"}},{"cell_type":"code","source":["# is_tensor 값을 넘기지 않으면, LongTensor 처리를 외부에서 따로 해줘야한다\n","def encode_bert(text, max_len, is_tensor=False):\n","  return tokenizer(text, padding=True, truncation=True, max_length=max_len, return_tensors= 'pt' if is_tensor else None)\n","\n","text = 'This movie was incredibly touching and had great performances!! I would definitely recommend it to anyone 🤔🤔.'\n","\n","print(f'딕셔너리 형태:\\n {encode_bert(text, 30)}')\n","print(f'PyTorch Tensor 형태:\\n {encode_bert(text, 30, True)}')\n","\n","# Tokenizer의 return 타입에 따라 input_ids, token_type_ids, attention_mask의 type이 다르다\n","# input_ids: input 값이 토큰화 된 것\n","# attention_mask: 실제 단어: 1 / padding: 0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0T64z8TJlFNr","executionInfo":{"status":"ok","timestamp":1743532699419,"user_tz":-540,"elapsed":15,"user":{"displayName":"표표표표푱","userId":"15791434277334549064"}},"outputId":"71a4f36b-9201-4450-e542-4225b363cafa"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["딕셔너리 형태:\n"," {'input_ids': [101, 2023, 3185, 2001, 11757, 7244, 1998, 2018, 2307, 4616, 999, 999, 1045, 2052, 5791, 16755, 2009, 2000, 3087, 100, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n","PyTorch Tensor 형태:\n"," {'input_ids': tensor([[  101,  2023,  3185,  2001, 11757,  7244,  1998,  2018,  2307,  4616,\n","           999,   999,  1045,  2052,  5791, 16755,  2009,  2000,  3087,   100,\n","          1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"]}]},{"cell_type":"markdown","source":["### 배치 데이터 정리 함수\n","- DataLoader가 train_ds에서 mini_batch 크기 만큼 샘플을 뽑는다\n","- collate_fn(batch)에 뽑은 샘플을 전달\n","- collate_fn\n","  - text 추출 : PyTorch Tensor 형태로 리턴 받음\n","  - label 추출 : LongTensor를 통해 PyTorch Tensor로 추출한 label type 변경\n","- (texts, label) 튜플 반환\n","  - `train_loader`\n","  - `test_loader`"],"metadata":{"id":"vlDNcS76ZUIT"}},{"cell_type":"code","source":["def collate_fn(batch):\n","  max_len = 400\n","  texts, labels = [], []\n","  for row in batch:\n","    labels.append(row['label'])\n","    texts.append(row['text'])\n","\n","  # texts = torch.LongTensor(encode_bert(texts, max_len).input_ids)\n","  # labels = torch.LongTensor(labels)\n","\n","  # encode_bert 함수에서 PyTorch Tensor type으로 바로 return했기에, LongTensor 처리가 불필요하다\n","  texts = encode_bert(texts, max_len, True).input_ids\n","  labels = torch.LongTensor(labels)\n","\n","  return texts, labels\n","\n","mini_batch=64\n","\n","train_loader = DataLoader(train_ds, mini_batch, shuffle=True, collate_fn=collate_fn)\n","test_loader = DataLoader(test_ds, mini_batch, shuffle=False, collate_fn=collate_fn)\n","\n","# for inputs, labels in train_loader:\n","#   print(inputs.shape)\n","#   print(labels.shape)\n","#   print(inputs)\n","#   print(labels)\n","#   break\n","\n","# train_loader 내부를 한 배치만 보기\n","inputs, labels = next(iter(train_loader))\n","print(f'input size: {inputs.shape}\\n')\n","print(f'label size: {labels.shape}\\n')\n","print(f'input[0]: {inputs[0]}\\n')\n","print(f'decode input[0]:\\n {tokenizer.decode(inputs[0])}')\n","print(f\"emotion of decode input[0]:\\n {'부정' if labels[0].item() == 0 else '긍정'}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZCcmtg4eZR2-","executionInfo":{"status":"ok","timestamp":1743536266596,"user_tz":-540,"elapsed":69,"user":{"displayName":"표표표표푱","userId":"15791434277334549064"}},"outputId":"8df63f77-a4d5-419d-8d1d-649fbd0c861e"},"execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":["input size: torch.Size([64, 400])\n","\n","label size: torch.Size([64])\n","\n","input[0]: tensor([  101,  2023,  2003,  2056,  2000,  2022,  1037,  3167,  2143,  2005,\n","         2848, 22132,  5280, 18891, 10649,  1012,  2002,  2241,  2009,  2006,\n","         2010,  2166,  2021,  2904,  2477,  2105,  2000,  4906,  1996,  3494,\n","         1010,  2040,  2024, 18145,  1012,  2122, 18145,  3058,  3376,  4275,\n","         1998,  2031,  2053,  3291,  2893,  2068,  1012,  4165,  2062,  2066,\n","         1037, 19965, 18286, 12127,  2084,  1037,  6317,  1010,  2987,  1005,\n","         1056,  2009,  1029,  2023,  2972,  3185,  2001,  2517,  2011,  2848,\n","         1010,  1998,  2009,  3065,  2129,  2041,  1997,  3543,  2007,  2613,\n","         2111,  2002,  2001,  1012,  2017,  1005,  2128,  4011,  2000,  4339,\n","         2054,  2017,  2113,  1010,  1998,  2002,  2106,  2008,  1010,  5262,\n","         1012,  1998,  3727,  1996,  4378, 11471,  1998,  5457,  1010,  1998,\n","         9981,  1010,  2005,  2008,  3043,  1012,  2023,  2003,  1037, 12731,\n","         9488,  2005,  2111,  2040,  2215,  2000,  2156,  9984,  2358,  8609,\n","         6528,  1010,  2040,  2001,  7129,  2157,  2044,  7467,  1012,  2021,\n","        22732, 17179,  1010,  2040,  2052,  1010,  1999,  2613,  2166,  1010,\n","         5914,  6766,  9712,  1010,  2001,  2036,  1037,  2944,  1010,  2066,\n","         2358,  8609,  6528,  1010,  2021,  2003,  1037,  2843,  2488,  1998,\n","         2038,  1037,  2062, 20851,  2112,  1012,  1999,  2755,  1010,  2358,\n","         8609,  6528,  1005,  1055,  2112,  2790,  3140,  1025,  2794,  1012,\n","         2016,  2987,  1005,  1056,  2031,  1037,  2843,  2000,  2079,  2007,\n","         1996,  2466,  1010,  2029,  2003,  3492,  9530,  6767,  7630,  3064,\n","         2000,  4088,  2007,  1012,  2035,  1999,  2035,  1010,  2296,  2839,\n","         1999,  2023,  2143,  2003,  8307,  2008,  2200,  2261,  2111,  2064,\n","        14396,  2007,  1010,  4983,  2017,  1005,  2128, 19965,  2013,  7128,\n","         2007,  3376,  3565,  5302,  9247,  2015,  2012,  2115, 10272,  2239,\n","         2655,  1012,  2005,  1996,  2717,  1997,  2149,  1010,  2009,  1005,\n","         1055,  2019, 29348,  1055, 12131,  2063, 17037,  1012,  2008,  1005,\n","         1055,  2054,  6433,  2043,  2017,  1005,  2128,  2041,  1997,  3543,\n","         1012,  2017, 20432,  2115,  2261,  2814,  2007,  2503, 13198,  1010,\n","         1998,  8501,  2035,  1996,  2717,  1012,   102,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n","\n","decode input[0]:\n"," [CLS] this is said to be a personal film for peter bogdonavitch. he based it on his life but changed things around to fit the characters, who are detectives. these detectives date beautiful models and have no problem getting them. sounds more like a millionaire playboy filmmaker than a detective, doesn ' t it? this entire movie was written by peter, and it shows how out of touch with real people he was. you ' re supposed to write what you know, and he did that, indeed. and leaves the audience bored and confused, and jealous, for that matter. this is a curio for people who want to see dorothy stratten, who was murdered right after filming. but patti hanson, who would, in real life, marry keith richards, was also a model, like stratten, but is a lot better and has a more ample part. in fact, stratten ' s part seemed forced ; added. she doesn ' t have a lot to do with the story, which is pretty convoluted to begin with. all in all, every character in this film is somebody that very few people can relate with, unless you ' re millionaire from manhattan with beautiful supermodels at your beckon call. for the rest of us, it ' s an irritating snore fest. that ' s what happens when you ' re out of touch. you entertain your few friends with inside jokes, and bore all the rest. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","emotion of decode input[0]:\n"," 부정\n"]}]},{"cell_type":"markdown","metadata":{"id":"i-FshZcTZBQ2"},"source":["\n","## Self-attention\n","\n","이번에는 self-attention을 구현해보겠습니다.\n","Self-attention은 shape이 (B, S, D)인 embedding이 들어왔을 때 attention을 적용하여 새로운 representation을 만들어내는 module입니다.\n","여기서 B는 batch size, S는 sequence length, D는 embedding 차원입니다.\n","구현은 다음과 같습니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MBlMVMZcRAxv"},"outputs":[],"source":["from torch import nn\n","from math import sqrt\n","\n","\n","class SelfAttention(nn.Module):\n","  def __init__(self, input_dim, d_model):\n","    super().__init__()\n","\n","    self.input_dim = input_dim\n","    self.d_model = d_model\n","\n","    self.wq = nn.Linear(input_dim, d_model)\n","    self.wk = nn.Linear(input_dim, d_model)\n","    self.wv = nn.Linear(input_dim, d_model)\n","    self.dense = nn.Linear(d_model, d_model)\n","\n","    self.softmax = nn.Softmax(dim=-1)\n","\n","  def forward(self, x, mask):\n","    q, k, v = self.wq(x), self.wk(x), self.wv(x)\n","    score = torch.matmul(q, k.transpose(-1, -2)) # (B, S, D) * (B, D, S) = (B, S, S)\n","    score = score / sqrt(self.d_model)\n","\n","    if mask is not None:\n","      score = score + (mask * -1e9)\n","\n","    score = self.softmax(score)\n","    result = torch.matmul(score, v)\n","    result = self.dense(result)\n","\n","    return result"]},{"cell_type":"markdown","metadata":{"id":"-S0vMp85ZRNO"},"source":["대부분은 Transformer 챕터에서 배운 수식들을 그대로 구현한 것에 불과합니다.\n","차이점은 `mask`의 존재여부입니다.\n","이전 챕터에서 우리는 가변적인 text data들에 padding token을 붙여 하나의 matrix로 만든 방법을 배웠습니다.\n","실제 attention 계산에서는 이를 무시해주기 위해 mask를 만들어 제공해주게 됩니다.\n","여기서 mask의 shape은 (B, S, 1)로, 만약 `mask[i, j] = True`이면 그 변수는 padding token에 해당한다는 뜻입니다.\n","이러한 값들을 무시해주는 방법은 shape이 (B, S, S)인 `score`가 있을 때(수업에서 배운 $A$와 동일) `score[i, j]`에 아주 작은 값을 더해주면 됩니다. 아주 작은 값은 예를 들어 `-1000..00 = -1e9` 같은 것이 있습니다.\n","이렇게 작은 값을 더해주고 나면 softmax를 거쳤을 때 0에 가까워지기 때문에 weighted sum 과정에서 padding token에 해당하는 `v` 값들을 무시할 수 있게 됩니다.\n","\n","다음은 self-attention과 feed-forward layer를 구현한 모습입니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VZHPCn9AS5Gp"},"outputs":[],"source":["class TransformerLayer(nn.Module):\n","  def __init__(self, input_dim, d_model, dff):\n","    super().__init__()\n","\n","    self.input_dim = input_dim\n","    self.d_model = d_model\n","    self.dff = dff\n","\n","    self.sa = SelfAttention(input_dim, d_model)\n","    self.ffn = nn.Sequential(\n","      nn.Linear(d_model, dff),\n","      nn.ReLU(),\n","      nn.Linear(dff, d_model)\n","    )\n","\n","  def forward(self, x, mask):\n","    x = self.sa(x, mask)\n","    x = self.ffn(x)\n","\n","    return x"]},{"cell_type":"markdown","metadata":{"id":"O_xC9BQJaU4q"},"source":["보시다시피 self-attention의 구현이 어렵지, Transformer layer 하나 구현하는 것은 수업 때 다룬 그림과 크게 구분되지 않는다는 점을 알 수 있습니다."]},{"cell_type":"markdown","metadata":{"id":"J3VYrqTJagS1"},"source":["## Positional encoding\n","\n","이번에는 positional encoding을 구현합니다. Positional encoding의 식은 다음과 같습니다:\n","$$\n","\\begin{align*} PE_{pos, 2i} &= \\sin\\left( \\frac{pos}{10000^{2i/D}} \\right), \\\\ PE_{pos, 2i+1} &= \\cos\\left( \\frac{pos}{10000^{2i/D}} \\right).\\end{align*}\n","$$\n","\n","이를 Numpy로 구현하여 PyTorch tensor로 변환한 모습은 다음과 같습니다:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1743317962830,"user":{"displayName":"조승혁","userId":"15759752471844115325"},"user_tz":-540},"id":"Uf_jMQWDUR79","outputId":"f40cd4e7-81ec-43b8-fede-d054f8dae962"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 400, 256])\n"]}],"source":["import numpy as np\n","\n","\n","def get_angles(pos, i, d_model):\n","    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n","    return pos * angle_rates\n","\n","def positional_encoding(position, d_model):\n","    angle_rads = get_angles(np.arange(position)[:, None], np.arange(d_model)[None, :], d_model)\n","    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n","    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n","    pos_encoding = angle_rads[None, ...]\n","\n","    return torch.FloatTensor(pos_encoding)\n","\n","\n","max_len = 400\n","print(positional_encoding(max_len, 256).shape)"]},{"cell_type":"markdown","metadata":{"id":"5unoDcBva3eN"},"source":["Positional encoding은 `angle_rads`를 구현하는 과정에서 모두 구현이 되었습니다. 여기서 `angle_rads`의 shape은 (S, D)입니다.\n","우리는 일반적으로 batch로 주어지는 shape이 (B, S, D)인 tensor를 다루기 때문에 마지막에 None을 활용하여 shape을 (1, S, D)로 바꿔주게됩니다.\n","\n","위에서 구현한 `TransformerLayer`와 positional encoding을 모두 합친 모습은 다음과 같습니다:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8MaiCGh8TsDH"},"outputs":[],"source":["class TextClassifier(nn.Module):\n","  def __init__(self, vocab_size, d_model, n_layers, dff):\n","    super().__init__()\n","\n","    self.vocab_size = vocab_size\n","    self.d_model = d_model\n","    self.n_layers = n_layers\n","    self.dff = dff\n","\n","    self.embedding = nn.Embedding(vocab_size, d_model)\n","    self.pos_encoding = nn.parameter.Parameter(positional_encoding(max_len, d_model), requires_grad=False)\n","    self.layers = nn.ModuleList([TransformerLayer(d_model, d_model, dff) for _ in range(n_layers)])\n","    self.classification = nn.Linear(d_model, 1)\n","\n","  def forward(self, x):\n","    mask = (x == tokenizer.pad_token_id)\n","    mask = mask[:, None, :]\n","    seq_len = x.shape[1]\n","\n","    x = self.embedding(x)\n","    x = x * sqrt(self.d_model)\n","    x = x + self.pos_encoding[:, :seq_len]\n","\n","    for layer in self.layers:\n","      x = layer(x, mask)\n","\n","    x = x[:, 0]\n","    x = self.classification(x)\n","\n","    return x\n","\n","\n","model = TextClassifier(len(tokenizer), 32, 2, 32)"]},{"cell_type":"markdown","metadata":{"id":"XXpjPWHjbUK8"},"source":["기존과 다른 점들은 다음과 같습니다:\n","1. `nn.ModuleList`를 사용하여 여러 layer의 구현을 쉽게 하였습니다.\n","2. Embedding, positional encoding, transformer layer를 거치고 난 후 마지막 label을 예측하기 위해 사용한 값은 `x[:, 0]`입니다. 기존의 RNN에서는 padding token을 제외한 마지막 token에 해당하는 representation을 사용한 것과 다릅니다. 이렇게 사용할 수 있는 이유는 attention 과정을 보시면 첫 번째 token에 대한 representation은 이후의 모든 token의 영향을 받습니다. 즉, 첫 번째 token 또한 전체 문장을 대변하는 의미를 가지고 있다고 할 수 있습니다. 그래서 일반적으로 Transformer를 text 분류에 사용할 때는 이와 같은 방식으로 구현됩니다."]},{"cell_type":"markdown","metadata":{"id":"QDq05OlAb2lB"},"source":["## 학습\n","\n","학습하는 코드는 기존 실습들과 동일하기 때문에 마지막 결과만 살펴보도록 하겠습니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YHVVsWBPQmnv"},"outputs":[],"source":["from torch.optim import Adam\n","\n","lr = 0.001\n","model = model.to('cuda')\n","loss_fn = nn.BCEWithLogitsLoss()\n","\n","optimizer = Adam(model.parameters(), lr=lr)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r88BALxO1zc1"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","\n","def accuracy(model, dataloader):\n","  cnt = 0\n","  acc = 0\n","\n","  for data in dataloader:\n","    inputs, labels = data\n","    inputs, labels = inputs.to('cuda'), labels.to('cuda')\n","\n","    preds = model(inputs)\n","    # preds = torch.argmax(preds, dim=-1)\n","    preds = (preds > 0).long()[..., 0]\n","\n","    cnt += labels.shape[0]\n","    acc += (labels == preds).sum().item()\n","\n","  return acc / cnt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":775},"executionInfo":{"elapsed":53654,"status":"error","timestamp":1743318016738,"user":{"displayName":"조승혁","userId":"15759752471844115325"},"user_tz":-540},"id":"al_b56TYRILq","outputId":"ecc7b1fa-5648-4277-da25-8fc436144ff0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch   0 | Train Loss: 7.381510192528367\n","=========> Train acc: 1.000 | Test acc: 1.000\n","Epoch   1 | Train Loss: 0.004828333854675293\n","=========> Train acc: 1.000 | Test acc: 1.000\n","Epoch   2 | Train Loss: 1.4901161193847656e-08\n","=========> Train acc: 1.000 | Test acc: 1.000\n","Epoch   3 | Train Loss: 1.4901161193847656e-08\n","=========> Train acc: 1.000 | Test acc: 1.000\n","Epoch   4 | Train Loss: 1.4901161193847656e-08\n","=========> Train acc: 1.000 | Test acc: 1.000\n","Epoch   5 | Train Loss: 1.4901161193847656e-08\n","=========> Train acc: 1.000 | Test acc: 1.000\n","Epoch   6 | Train Loss: 1.4901161193847656e-08\n","=========> Train acc: 1.000 | Test acc: 1.000\n","Epoch   7 | Train Loss: 2.8049244704675402e-08\n","=========> Train acc: 1.000 | Test acc: 1.000\n","Epoch   8 | Train Loss: 1.4901161193847656e-08\n","=========> Train acc: 1.000 | Test acc: 1.000\n","Epoch   9 | Train Loss: 1.4901161193847656e-08\n","=========> Train acc: 1.000 | Test acc: 1.000\n","Epoch  10 | Train Loss: 1.4901161193847656e-08\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-d4e5ab8584c3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"=========> Train acc: {train_acc:.3f} | Test acc: {test_acc:.3f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-cf502b42c7d3>\u001b[0m in \u001b[0;36maccuracy\u001b[0;34m(model, dataloader)\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-2-fb258e4edfe8>\u001b[0m in \u001b[0;36mcollate_fn\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mtexts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m   \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m   \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2885\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_target_context_manager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2886\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_input_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2887\u001b[0;31m             \u001b[0mencodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mall_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2888\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtext_target\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2889\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_target_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   2973\u001b[0m                 )\n\u001b[1;32m   2974\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2975\u001b[0;31m             return self.batch_encode_plus(\n\u001b[0m\u001b[1;32m   2976\u001b[0m                 \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2977\u001b[0m                 \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   3175\u001b[0m         )\n\u001b[1;32m   3176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3177\u001b[0;31m         return self._batch_encode_plus(\n\u001b[0m\u001b[1;32m   3178\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3179\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens)\u001b[0m\n\u001b[1;32m    537\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_special_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_special_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m         encodings = self._tokenizer.encode_batch(\n\u001b[0m\u001b[1;32m    540\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["n_epochs = 5\n","\n","for epoch in range(n_epochs):\n","  total_loss = 0.\n","  model.train()\n","  for data in train_loader:\n","    model.zero_grad()\n","    inputs, labels = data\n","    inputs, labels = inputs.to('cuda'), labels.to('cuda').float()\n","\n","    preds = model(inputs)[..., 0]\n","    loss = loss_fn(preds, labels)\n","    loss.backward()\n","    optimizer.step()\n","\n","    total_loss += loss.item()\n","\n","  print(f\"Epoch {epoch:3d} | Train Loss: {total_loss}\")\n","\n","  with torch.no_grad():\n","    model.eval()\n","    train_acc = accuracy(model, train_loader)\n","    test_acc = accuracy(model, test_loader)\n","    print(f\"=========> Train acc: {train_acc:.3f} | Test acc: {test_acc:.3f}\")"]},{"cell_type":"code","source":[],"metadata":{"id":"u-I121osywgW"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1vh4ALpRVicq9hdonHYiLTlsPhY6LLZAE","timestamp":1743433609631}],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":0}