{"cells":[{"cell_type":"markdown","metadata":{"id":"ymxatB5WYxlL"},"source":["# Transformer - ê¸°ë³¸ ê³¼ì œ\n","> ì£¼ì–´ì§„ ë¬¸ìž¥ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ì„ ë§Œë“¤ì–´ë³¸ë‹¤\n","   \n","\n","\n","### RNN vs Transformer\n","| í•­ëª©             | RNN (Recurrent Neural Network)                             | Transformer                                               |\n","|:------------------|-------------------------------------------------------------|------------------------------------------------------------|\n","| **ê¸°ë³¸ êµ¬ì¡°**     | ìˆœì°¨ì ìœ¼ë¡œ ì‹œí€€ìŠ¤ë¥¼ ì²˜ë¦¬ (ì‹œê°„ ìˆœì„œëŒ€ë¡œ)                  | ì „ì²´ ì‹œí€€ìŠ¤ë¥¼ ë™ì‹œì— ì²˜ë¦¬ (ë³‘ë ¬ì²˜ë¦¬ ê°€ëŠ¥)                  |\n","| **ë³‘ë ¬ ì²˜ë¦¬**     | âŒ ë¶ˆê°€ëŠ¥ (ì´ì „ ë‹¨ê³„ ê²°ê³¼ê°€ ë‹¤ìŒ ë‹¨ê³„ ìž…ë ¥ì— í•„ìš”)         | âœ… ê°€ëŠ¥ (Self-Attentionìœ¼ë¡œ ëª¨ë“  ìœ„ì¹˜ë¥¼ ë™ì‹œì— ì°¸ì¡°)       |\n","| **ìž¥ê¸° ì˜ì¡´ì„± ì²˜ë¦¬** | âŒ ì–´ë ¤ì›€ (Vanishing Gradient ë¬¸ì œ)                      | âœ… í›¨ì”¬ ê°•ë ¥í•¨ (ëª¨ë“  ë‹¨ì–´ ê°„ ê´€ê³„ íŒŒì•… ê°€ëŠ¥)               |\n","| **ìž…ë ¥ ìœ„ì¹˜ ì •ë³´** | ìžì—°ìŠ¤ëŸ½ê²Œ ìˆœì„œë¥¼ ë”°ë¦„                                    | ë³„ë„ë¡œ Positional Encoding í•„ìš”                           |\n","| **ëŒ€í‘œ ëª¨ë¸**     | LSTM, GRU                                                  | BERT, GPT, T5, etc.                                       |\n","| **ì„±ëŠ¥ ë° ì†ë„**  | ê¸´ ë¬¸ìž¥ ì²˜ë¦¬ì— ì•½í•¨ / ëŠë¦¼                                 | ê¸´ ë¬¸ìž¥ë„ ìž˜ ì²˜ë¦¬ / ë¹ ë¦„ (GPU ë³‘ë ¬í™”)                      |\n","\n","\n","   \n","### IMDb ê°ì„± ë¶„ë¥˜ ë°ì´í„°ì…‹\n","- Internet Movie Database\n","- ê°ì„± ë¶„ë¥˜(Sentiment Classification)\n","  - ì˜í™”ì— ëŒ€í•œ ì •ë³´ / ì‚¬ìš©ìž ë¦¬ë·°(text) / ë³„ì \n","  - ìž…ë ¥: ì‚¬ìš©ìž ë¦¬ë·°(text)\n","  - ì¶œë ¥: ê°ì„± ë¼ë²¨(ê¸ì • or ë¶€ì •)\n","- í…ìŠ¤íŠ¸ ë¶„ë¥˜(Binary Classification) ë¬¸ì œ\n","\n","**Example**\n","``` makefile\n","ìž…ë ¥: \"This movie was surprisingly good and well-acted.\"\n","ì¶œë ¥: 1 (ê¸ì •)\n","```\n","\n","``` makefile\n","ìž…ë ¥: \"I wasted two hours of my life watching this.\"\n","ì¶œë ¥: 0 (ë¶€ì •)\n","```\n","\n","### import ëª©ë¡\n","- `datasets`: Hugging Faceì˜ ë°ì´í„°ì…‹ ë¡œë” ë¼ì´ë¸ŒëŸ¬ë¦¬ / IMDb, MNIST ë“± ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ í™œìš© ê°€ëŠ¥\n","- `sacremoses`: í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ëª¨ë“ˆ / tokenizer ë‚´ë¶€ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìžˆìŒ\n"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"1X7RM2du1zcr","outputId":"9b5b693f-8bb3-4f4d-9d7b-adfb3a0a4598","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1743529946959,"user_tz":-540,"elapsed":4337,"user":{"displayName":"í‘œí‘œí‘œí‘œí‘±","userId":"15791434277334549064"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting datasets\n","  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n","Collecting sacremoses\n","  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess<0.70.17 (from datasets)\n","  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n","Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n","  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.14)\n","Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.29.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacremoses) (2024.11.6)\n","Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from sacremoses) (8.1.8)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from sacremoses) (1.4.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n","Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: xxhash, sacremoses, fsspec, dill, multiprocess, datasets\n","  Attempting uninstall: fsspec\n","    Found existing installation: fsspec 2025.3.0\n","    Uninstalling fsspec-2025.3.0:\n","      Successfully uninstalled fsspec-2025.3.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.12.0 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 sacremoses-0.1.1 xxhash-3.5.0\n"]}],"source":["!pip install datasets sacremoses"]},{"cell_type":"markdown","source":["# **Attention is all you need**   \n","![Attention is all you need](https://binarymindset.com/wp-content/uploads/2023/08/transformers-4-574x1024.png)   \n","- 6 encoder - 6 decoder structure\n","\n","## Transformer êµ¬ì¡°\n","\n","### 1ï¸âƒ£Embedding   \n","### 2ï¸âƒ£Positional Encoding   \n","### 3ï¸âƒ£Encoder   \n","- Self-Attention\n","- Feed Forward\n","\n","### 4ï¸âƒ£Decoder  \n","- Masked Self-Attention\n","- Encoder-Decoder Attention\n","- Feed Forward\n","\n","### 5ï¸âƒ£Prediction   \n","\n"],"metadata":{"id":"aOck_fe_E120"}},{"cell_type":"markdown","source":["### Load Tokenizer from Hugging Face\n","\n","**BERT vs GPT2 Tokenizer | AutoTokenizer** ì •ë¦¬  \n","[![Notebook](https://img.shields.io/badge/Jupyter-Notebook-orange?logo=jupyter)](https://nbviewer.org/github/zerovodka/ML-learning/blob/master/src/week2/BERT-vs-GPT-Tokenizer.ipynb?flush_cache=true)\n","- Emoji ì²˜ë¦¬ ê´€ì ì—ì„œ ë¹„êµ\n","- AutoTokenizer\n","- ì‹¤ë¬´ vs ì‹¤í—˜ ì ì ˆí•œ import\n","- tokenizer ë‚´ë¶€ ì„±ë¶„ ê°’ ì„¤ëª…\n"],"metadata":{"id":"Ybp8TvFAZacg"}},{"cell_type":"code","execution_count":3,"metadata":{"id":"HOdhoBVA1zcu","executionInfo":{"status":"ok","timestamp":1743529998805,"user_tz":-540,"elapsed":5,"user":{"displayName":"í‘œí‘œí‘œí‘œí‘±","userId":"15791434277334549064"}}},"outputs":[],"source":["import torch\n","from datasets import load_dataset\n","from torch.utils.data import DataLoader\n","from transformers import BertTokenizerFast\n","from tokenizers import (\n","    decoders,\n","    models,\n","    normalizers,\n","    pre_tokenizers,\n","    processors,\n","    trainers,\n","    Tokenizer,\n",")"]},{"cell_type":"markdown","source":["**Load BERT Tokenizer**   \n","- BERT Tokenizer Modelì˜ ì‚¬ì „ í•™ìŠµëœ êµ¬ì¡°, ì„¤ì •, vocabularyë¥¼ í™•ì¸í•  ìˆ˜ ìžˆë‹¤"],"metadata":{"id":"7eAIcMnCfyUI"}},{"cell_type":"code","source":["# ds = load_dataset(\"stanfordnlp/imdb\")\n","\n","# imdb ë°ì´í„° ì•žì—ì„œ 5%ë§Œ ìž˜ë¼ì„œ ìˆ˜í–‰: ë¹ ë¥¸ ì‹¤í—˜ì„ ìœ„í•¨\n","train_ds = load_dataset(\"stanfordnlp/imdb\", split=\"train[:5%]\")\n","test_ds = load_dataset(\"stanfordnlp/imdb\", split=\"test[:5%]\")\n","\n","tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n","\n","print(tokenizer)\n","\n","print(f'ì‚¬ìš©ì¤‘ì¸ tokenizer ëª¨ë¸: \\n{tokenizer.name_or_path}')\n","print(f'\\n\\n{tokenizer.name_or_path} ëª¨ë¸ì˜ vacabulary size: \\n{tokenizer.vocab_size}')\n","print(f'\\n\\n{tokenizer.name_or_path} ëª¨ë¸ì˜ max length: \\n{tokenizer.model_max_length}')\n","print(f'\\n\\n{tokenizer.name_or_path} ëª¨ë¸ì˜ special_tokens_map: \\n{tokenizer.special_tokens_map}')\n","print(f'\\n\\n{tokenizer.name_or_path} ëª¨ë¸ì˜ decode ì‹œ íŠ¹ìˆ˜ token ì²˜ë¦¬ ë°©ì‹: \\n{tokenizer.added_tokens_decoder}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ACgKfXLSfIXR","executionInfo":{"status":"ok","timestamp":1743531525585,"user_tz":-540,"elapsed":2414,"user":{"displayName":"í‘œí‘œí‘œí‘œí‘±","userId":"15791434277334549064"}},"outputId":"b598cd23-59c5-42c4-a847-ab701c41a286"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n","\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}\n",")\n","ì‚¬ìš©ì¤‘ì¸ tokenizer ëª¨ë¸: \n","bert-base-uncased\n","\n","\n","bert-base-uncased ëª¨ë¸ì˜ vacabulary size: \n","30522\n","\n","\n","bert-base-uncased ëª¨ë¸ì˜ max length: \n","512\n","\n","\n","bert-base-uncased ëª¨ë¸ì˜ special_tokens_map: \n","{'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}\n","\n","\n","bert-base-uncased ëª¨ë¸ì˜ decode ì‹œ íŠ¹ìˆ˜ token ì²˜ë¦¬ ë°©ì‹: \n","{0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True), 100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True), 101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True), 102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True), 103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True)}\n"]}]},{"cell_type":"markdown","source":["**Encode function**\n","- tokenizer ëª¨ë¸ ë§ˆë‹¤ ì„¤ì • ê°’ì´ ë‹¤ë¥´ê¸°ë•Œë¬¸ì—, ê°ê°ì— ë§žëŠ” encode í•¨ìˆ˜ë¥¼ ì„ ì–¸í•œë‹¤\n","\n","**ê¸°ëŠ¥**\n","- tokenizing: `text` -> token ìª¼ê°œê¸°\n","- encoding: token -> number ID ì¸ì½”ë”©\n","- padding / truncation: íŒ¨ë”©, ê¸¸ì´ ìž˜ë¼ë‚´ê¸°(max length ì„ ì–¸ ê¸°ì¤€)\n","- return: ë”•ì…”ë„ˆë¦¬ ë¦¬í„´\n","  - `return_tensor='pt'` ì†ì„± ì¶”ê°€ ì‹œ: python tensor ë¦¬í„´   \n","\n","|ì˜µì…˜|ì„¤ëª…|\n","|:---|:---|\n","|**`return_tensors='pt'`**|\tPyTorch í…ì„œë¡œ ë¦¬í„´ â†’ ëª¨ë¸ ìž…ë ¥ì— ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥|\n","|**`return_tensors='tf'`**|\tTensorFlowìš©|\n","|**`return_tensors='np'`**|\tNumPyìš©\n","|**`return_tensors=None`**|\tíŒŒì´ì¬ dict (list í˜•íƒœ) â†’ ë””ë²„ê¹…/ì¶œë ¥ í™•ì¸ì— ìœ ë¦¬"],"metadata":{"id":"63tvIaxFlXU2"}},{"cell_type":"code","source":["# is_tensor ê°’ì„ ë„˜ê¸°ì§€ ì•Šìœ¼ë©´, LongTensor ì²˜ë¦¬ë¥¼ ì™¸ë¶€ì—ì„œ ë”°ë¡œ í•´ì¤˜ì•¼í•œë‹¤\n","def encode_bert(text, max_len, is_tensor=False):\n","  return tokenizer(text, padding=True, truncation=True, max_length=max_len, return_tensors= 'pt' if is_tensor else None)\n","\n","text = 'This movie was incredibly touching and had great performances!! I would definitely recommend it to anyone ðŸ¤”ðŸ¤”.'\n","\n","print(f'ë”•ì…”ë„ˆë¦¬ í˜•íƒœ:\\n {encode_bert(text, 30)}')\n","print(f'PyTorch Tensor í˜•íƒœ:\\n {encode_bert(text, 30, True)}')\n","\n","# Tokenizerì˜ return íƒ€ìž…ì— ë”°ë¼ input_ids, token_type_ids, attention_maskì˜ typeì´ ë‹¤ë¥´ë‹¤\n","# input_ids: input ê°’ì´ í† í°í™” ëœ ê²ƒ\n","# attention_mask: ì‹¤ì œ ë‹¨ì–´: 1 / padding: 0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0T64z8TJlFNr","executionInfo":{"status":"ok","timestamp":1743532699419,"user_tz":-540,"elapsed":15,"user":{"displayName":"í‘œí‘œí‘œí‘œí‘±","userId":"15791434277334549064"}},"outputId":"71a4f36b-9201-4450-e542-4225b363cafa"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["ë”•ì…”ë„ˆë¦¬ í˜•íƒœ:\n"," {'input_ids': [101, 2023, 3185, 2001, 11757, 7244, 1998, 2018, 2307, 4616, 999, 999, 1045, 2052, 5791, 16755, 2009, 2000, 3087, 100, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n","PyTorch Tensor í˜•íƒœ:\n"," {'input_ids': tensor([[  101,  2023,  3185,  2001, 11757,  7244,  1998,  2018,  2307,  4616,\n","           999,   999,  1045,  2052,  5791, 16755,  2009,  2000,  3087,   100,\n","          1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"]}]},{"cell_type":"markdown","source":["### ë°°ì¹˜ ë°ì´í„° ì •ë¦¬ í•¨ìˆ˜\n","- DataLoaderê°€ train_dsì—ì„œ mini_batch í¬ê¸° ë§Œí¼ ìƒ˜í”Œì„ ë½‘ëŠ”ë‹¤\n","- collate_fn(batch)ì— ë½‘ì€ ìƒ˜í”Œì„ ì „ë‹¬\n","- collate_fn\n","  - text ì¶”ì¶œ : PyTorch Tensor í˜•íƒœë¡œ ë¦¬í„´ ë°›ìŒ\n","  - label ì¶”ì¶œ : LongTensorë¥¼ í†µí•´ PyTorch Tensorë¡œ ì¶”ì¶œí•œ label type ë³€ê²½\n","- (texts, label) íŠœí”Œ ë°˜í™˜\n","  - `train_loader`\n","  - `test_loader`"],"metadata":{"id":"vlDNcS76ZUIT"}},{"cell_type":"code","source":["def collate_fn(batch):\n","  max_len = 400\n","  texts, labels = [], []\n","  for row in batch:\n","    labels.append(row['label'])\n","    texts.append(row['text'])\n","\n","  # texts = torch.LongTensor(encode_bert(texts, max_len).input_ids)\n","  # labels = torch.LongTensor(labels)\n","\n","  # encode_bert í•¨ìˆ˜ì—ì„œ PyTorch Tensor typeìœ¼ë¡œ ë°”ë¡œ returní–ˆê¸°ì—, LongTensor ì²˜ë¦¬ê°€ ë¶ˆí•„ìš”í•˜ë‹¤\n","  texts = encode_bert(texts, max_len, True).input_ids\n","  labels = torch.LongTensor(labels)\n","\n","  return texts, labels\n","\n","mini_batch=64\n","\n","train_loader = DataLoader(train_ds, mini_batch, shuffle=True, collate_fn=collate_fn)\n","test_loader = DataLoader(test_ds, mini_batch, shuffle=False, collate_fn=collate_fn)\n","\n","# for inputs, labels in train_loader:\n","#   print(inputs.shape)\n","#   print(labels.shape)\n","#   print(inputs)\n","#   print(labels)\n","#   break\n","\n","# train_loader ë‚´ë¶€ë¥¼ í•œ ë°°ì¹˜ë§Œ ë³´ê¸°\n","inputs, labels = next(iter(train_loader))\n","print(f'input size: {inputs.shape}\\n')\n","print(f'label size: {labels.shape}\\n')\n","print(f'input[0]: {inputs[0]}\\n')\n","print(f'decode input[0]:\\n {tokenizer.decode(inputs[0])}')\n","print(f\"emotion of decode input[0]:\\n {'ë¶€ì •' if labels[0].item() == 0 else 'ê¸ì •'}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZCcmtg4eZR2-","executionInfo":{"status":"ok","timestamp":1743536266596,"user_tz":-540,"elapsed":69,"user":{"displayName":"í‘œí‘œí‘œí‘œí‘±","userId":"15791434277334549064"}},"outputId":"8df63f77-a4d5-419d-8d1d-649fbd0c861e"},"execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":["input size: torch.Size([64, 400])\n","\n","label size: torch.Size([64])\n","\n","input[0]: tensor([  101,  2023,  2003,  2056,  2000,  2022,  1037,  3167,  2143,  2005,\n","         2848, 22132,  5280, 18891, 10649,  1012,  2002,  2241,  2009,  2006,\n","         2010,  2166,  2021,  2904,  2477,  2105,  2000,  4906,  1996,  3494,\n","         1010,  2040,  2024, 18145,  1012,  2122, 18145,  3058,  3376,  4275,\n","         1998,  2031,  2053,  3291,  2893,  2068,  1012,  4165,  2062,  2066,\n","         1037, 19965, 18286, 12127,  2084,  1037,  6317,  1010,  2987,  1005,\n","         1056,  2009,  1029,  2023,  2972,  3185,  2001,  2517,  2011,  2848,\n","         1010,  1998,  2009,  3065,  2129,  2041,  1997,  3543,  2007,  2613,\n","         2111,  2002,  2001,  1012,  2017,  1005,  2128,  4011,  2000,  4339,\n","         2054,  2017,  2113,  1010,  1998,  2002,  2106,  2008,  1010,  5262,\n","         1012,  1998,  3727,  1996,  4378, 11471,  1998,  5457,  1010,  1998,\n","         9981,  1010,  2005,  2008,  3043,  1012,  2023,  2003,  1037, 12731,\n","         9488,  2005,  2111,  2040,  2215,  2000,  2156,  9984,  2358,  8609,\n","         6528,  1010,  2040,  2001,  7129,  2157,  2044,  7467,  1012,  2021,\n","        22732, 17179,  1010,  2040,  2052,  1010,  1999,  2613,  2166,  1010,\n","         5914,  6766,  9712,  1010,  2001,  2036,  1037,  2944,  1010,  2066,\n","         2358,  8609,  6528,  1010,  2021,  2003,  1037,  2843,  2488,  1998,\n","         2038,  1037,  2062, 20851,  2112,  1012,  1999,  2755,  1010,  2358,\n","         8609,  6528,  1005,  1055,  2112,  2790,  3140,  1025,  2794,  1012,\n","         2016,  2987,  1005,  1056,  2031,  1037,  2843,  2000,  2079,  2007,\n","         1996,  2466,  1010,  2029,  2003,  3492,  9530,  6767,  7630,  3064,\n","         2000,  4088,  2007,  1012,  2035,  1999,  2035,  1010,  2296,  2839,\n","         1999,  2023,  2143,  2003,  8307,  2008,  2200,  2261,  2111,  2064,\n","        14396,  2007,  1010,  4983,  2017,  1005,  2128, 19965,  2013,  7128,\n","         2007,  3376,  3565,  5302,  9247,  2015,  2012,  2115, 10272,  2239,\n","         2655,  1012,  2005,  1996,  2717,  1997,  2149,  1010,  2009,  1005,\n","         1055,  2019, 29348,  1055, 12131,  2063, 17037,  1012,  2008,  1005,\n","         1055,  2054,  6433,  2043,  2017,  1005,  2128,  2041,  1997,  3543,\n","         1012,  2017, 20432,  2115,  2261,  2814,  2007,  2503, 13198,  1010,\n","         1998,  8501,  2035,  1996,  2717,  1012,   102,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n","\n","decode input[0]:\n"," [CLS] this is said to be a personal film for peter bogdonavitch. he based it on his life but changed things around to fit the characters, who are detectives. these detectives date beautiful models and have no problem getting them. sounds more like a millionaire playboy filmmaker than a detective, doesn ' t it? this entire movie was written by peter, and it shows how out of touch with real people he was. you ' re supposed to write what you know, and he did that, indeed. and leaves the audience bored and confused, and jealous, for that matter. this is a curio for people who want to see dorothy stratten, who was murdered right after filming. but patti hanson, who would, in real life, marry keith richards, was also a model, like stratten, but is a lot better and has a more ample part. in fact, stratten ' s part seemed forced ; added. she doesn ' t have a lot to do with the story, which is pretty convoluted to begin with. all in all, every character in this film is somebody that very few people can relate with, unless you ' re millionaire from manhattan with beautiful supermodels at your beckon call. for the rest of us, it ' s an irritating snore fest. that ' s what happens when you ' re out of touch. you entertain your few friends with inside jokes, and bore all the rest. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","emotion of decode input[0]:\n"," ë¶€ì •\n"]}]},{"cell_type":"markdown","metadata":{"id":"i-FshZcTZBQ2"},"source":["\n","## Self-attention\n","\n","ì´ë²ˆì—ëŠ” self-attentionì„ êµ¬í˜„í•´ë³´ê² ìŠµë‹ˆë‹¤.\n","Self-attentionì€ shapeì´ (B, S, D)ì¸ embeddingì´ ë“¤ì–´ì™”ì„ ë•Œ attentionì„ ì ìš©í•˜ì—¬ ìƒˆë¡œìš´ representationì„ ë§Œë“¤ì–´ë‚´ëŠ” moduleìž…ë‹ˆë‹¤.\n","ì—¬ê¸°ì„œ BëŠ” batch size, SëŠ” sequence length, DëŠ” embedding ì°¨ì›ìž…ë‹ˆë‹¤.\n","êµ¬í˜„ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MBlMVMZcRAxv"},"outputs":[],"source":["from torch import nn\n","from math import sqrt\n","\n","\n","class SelfAttention(nn.Module):\n","  def __init__(self, input_dim, d_model):\n","    super().__init__()\n","\n","    self.input_dim = input_dim\n","    self.d_model = d_model\n","\n","    self.wq = nn.Linear(input_dim, d_model)\n","    self.wk = nn.Linear(input_dim, d_model)\n","    self.wv = nn.Linear(input_dim, d_model)\n","    self.dense = nn.Linear(d_model, d_model)\n","\n","    self.softmax = nn.Softmax(dim=-1)\n","\n","  def forward(self, x, mask):\n","    q, k, v = self.wq(x), self.wk(x), self.wv(x)\n","    score = torch.matmul(q, k.transpose(-1, -2)) # (B, S, D) * (B, D, S) = (B, S, S)\n","    score = score / sqrt(self.d_model)\n","\n","    if mask is not None:\n","      score = score + (mask * -1e9)\n","\n","    score = self.softmax(score)\n","    result = torch.matmul(score, v)\n","    result = self.dense(result)\n","\n","    return result"]},{"cell_type":"markdown","metadata":{"id":"-S0vMp85ZRNO"},"source":["ëŒ€ë¶€ë¶„ì€ Transformer ì±•í„°ì—ì„œ ë°°ìš´ ìˆ˜ì‹ë“¤ì„ ê·¸ëŒ€ë¡œ êµ¬í˜„í•œ ê²ƒì— ë¶ˆê³¼í•©ë‹ˆë‹¤.\n","ì°¨ì´ì ì€ `mask`ì˜ ì¡´ìž¬ì—¬ë¶€ìž…ë‹ˆë‹¤.\n","ì´ì „ ì±•í„°ì—ì„œ ìš°ë¦¬ëŠ” ê°€ë³€ì ì¸ text dataë“¤ì— padding tokenì„ ë¶™ì—¬ í•˜ë‚˜ì˜ matrixë¡œ ë§Œë“  ë°©ë²•ì„ ë°°ì› ìŠµë‹ˆë‹¤.\n","ì‹¤ì œ attention ê³„ì‚°ì—ì„œëŠ” ì´ë¥¼ ë¬´ì‹œí•´ì£¼ê¸° ìœ„í•´ maskë¥¼ ë§Œë“¤ì–´ ì œê³µí•´ì£¼ê²Œ ë©ë‹ˆë‹¤.\n","ì—¬ê¸°ì„œ maskì˜ shapeì€ (B, S, 1)ë¡œ, ë§Œì•½ `mask[i, j] = True`ì´ë©´ ê·¸ ë³€ìˆ˜ëŠ” padding tokenì— í•´ë‹¹í•œë‹¤ëŠ” ëœ»ìž…ë‹ˆë‹¤.\n","ì´ëŸ¬í•œ ê°’ë“¤ì„ ë¬´ì‹œí•´ì£¼ëŠ” ë°©ë²•ì€ shapeì´ (B, S, S)ì¸ `score`ê°€ ìžˆì„ ë•Œ(ìˆ˜ì—…ì—ì„œ ë°°ìš´ $A$ì™€ ë™ì¼) `score[i, j]`ì— ì•„ì£¼ ìž‘ì€ ê°’ì„ ë”í•´ì£¼ë©´ ë©ë‹ˆë‹¤. ì•„ì£¼ ìž‘ì€ ê°’ì€ ì˜ˆë¥¼ ë“¤ì–´ `-1000..00 = -1e9` ê°™ì€ ê²ƒì´ ìžˆìŠµë‹ˆë‹¤.\n","ì´ë ‡ê²Œ ìž‘ì€ ê°’ì„ ë”í•´ì£¼ê³  ë‚˜ë©´ softmaxë¥¼ ê±°ì³¤ì„ ë•Œ 0ì— ê°€ê¹Œì›Œì§€ê¸° ë•Œë¬¸ì— weighted sum ê³¼ì •ì—ì„œ padding tokenì— í•´ë‹¹í•˜ëŠ” `v` ê°’ë“¤ì„ ë¬´ì‹œí•  ìˆ˜ ìžˆê²Œ ë©ë‹ˆë‹¤.\n","\n","ë‹¤ìŒì€ self-attentionê³¼ feed-forward layerë¥¼ êµ¬í˜„í•œ ëª¨ìŠµìž…ë‹ˆë‹¤."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VZHPCn9AS5Gp"},"outputs":[],"source":["class TransformerLayer(nn.Module):\n","  def __init__(self, input_dim, d_model, dff):\n","    super().__init__()\n","\n","    self.input_dim = input_dim\n","    self.d_model = d_model\n","    self.dff = dff\n","\n","    self.sa = SelfAttention(input_dim, d_model)\n","    self.ffn = nn.Sequential(\n","      nn.Linear(d_model, dff),\n","      nn.ReLU(),\n","      nn.Linear(dff, d_model)\n","    )\n","\n","  def forward(self, x, mask):\n","    x = self.sa(x, mask)\n","    x = self.ffn(x)\n","\n","    return x"]},{"cell_type":"markdown","metadata":{"id":"O_xC9BQJaU4q"},"source":["ë³´ì‹œë‹¤ì‹œí”¼ self-attentionì˜ êµ¬í˜„ì´ ì–´ë µì§€, Transformer layer í•˜ë‚˜ êµ¬í˜„í•˜ëŠ” ê²ƒì€ ìˆ˜ì—… ë•Œ ë‹¤ë£¬ ê·¸ë¦¼ê³¼ í¬ê²Œ êµ¬ë¶„ë˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ì ì„ ì•Œ ìˆ˜ ìžˆìŠµë‹ˆë‹¤."]},{"cell_type":"markdown","metadata":{"id":"J3VYrqTJagS1"},"source":["## Positional encoding\n","\n","ì´ë²ˆì—ëŠ” positional encodingì„ êµ¬í˜„í•©ë‹ˆë‹¤. Positional encodingì˜ ì‹ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n","$$\n","\\begin{align*} PE_{pos, 2i} &= \\sin\\left( \\frac{pos}{10000^{2i/D}} \\right), \\\\ PE_{pos, 2i+1} &= \\cos\\left( \\frac{pos}{10000^{2i/D}} \\right).\\end{align*}\n","$$\n","\n","ì´ë¥¼ Numpyë¡œ êµ¬í˜„í•˜ì—¬ PyTorch tensorë¡œ ë³€í™˜í•œ ëª¨ìŠµì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1743317962830,"user":{"displayName":"ì¡°ìŠ¹í˜","userId":"15759752471844115325"},"user_tz":-540},"id":"Uf_jMQWDUR79","outputId":"f40cd4e7-81ec-43b8-fede-d054f8dae962"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 400, 256])\n"]}],"source":["import numpy as np\n","\n","\n","def get_angles(pos, i, d_model):\n","    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n","    return pos * angle_rates\n","\n","def positional_encoding(position, d_model):\n","    angle_rads = get_angles(np.arange(position)[:, None], np.arange(d_model)[None, :], d_model)\n","    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n","    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n","    pos_encoding = angle_rads[None, ...]\n","\n","    return torch.FloatTensor(pos_encoding)\n","\n","\n","max_len = 400\n","print(positional_encoding(max_len, 256).shape)"]},{"cell_type":"markdown","metadata":{"id":"5unoDcBva3eN"},"source":["Positional encodingì€ `angle_rads`ë¥¼ êµ¬í˜„í•˜ëŠ” ê³¼ì •ì—ì„œ ëª¨ë‘ êµ¬í˜„ì´ ë˜ì—ˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œ `angle_rads`ì˜ shapeì€ (S, D)ìž…ë‹ˆë‹¤.\n","ìš°ë¦¬ëŠ” ì¼ë°˜ì ìœ¼ë¡œ batchë¡œ ì£¼ì–´ì§€ëŠ” shapeì´ (B, S, D)ì¸ tensorë¥¼ ë‹¤ë£¨ê¸° ë•Œë¬¸ì— ë§ˆì§€ë§‰ì— Noneì„ í™œìš©í•˜ì—¬ shapeì„ (1, S, D)ë¡œ ë°”ê¿”ì£¼ê²Œë©ë‹ˆë‹¤.\n","\n","ìœ„ì—ì„œ êµ¬í˜„í•œ `TransformerLayer`ì™€ positional encodingì„ ëª¨ë‘ í•©ì¹œ ëª¨ìŠµì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8MaiCGh8TsDH"},"outputs":[],"source":["class TextClassifier(nn.Module):\n","  def __init__(self, vocab_size, d_model, n_layers, dff):\n","    super().__init__()\n","\n","    self.vocab_size = vocab_size\n","    self.d_model = d_model\n","    self.n_layers = n_layers\n","    self.dff = dff\n","\n","    self.embedding = nn.Embedding(vocab_size, d_model)\n","    self.pos_encoding = nn.parameter.Parameter(positional_encoding(max_len, d_model), requires_grad=False)\n","    self.layers = nn.ModuleList([TransformerLayer(d_model, d_model, dff) for _ in range(n_layers)])\n","    self.classification = nn.Linear(d_model, 1)\n","\n","  def forward(self, x):\n","    mask = (x == tokenizer.pad_token_id)\n","    mask = mask[:, None, :]\n","    seq_len = x.shape[1]\n","\n","    x = self.embedding(x)\n","    x = x * sqrt(self.d_model)\n","    x = x + self.pos_encoding[:, :seq_len]\n","\n","    for layer in self.layers:\n","      x = layer(x, mask)\n","\n","    x = x[:, 0]\n","    x = self.classification(x)\n","\n","    return x\n","\n","\n","model = TextClassifier(len(tokenizer), 32, 2, 32)"]},{"cell_type":"markdown","metadata":{"id":"XXpjPWHjbUK8"},"source":["ê¸°ì¡´ê³¼ ë‹¤ë¥¸ ì ë“¤ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n","1. `nn.ModuleList`ë¥¼ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ layerì˜ êµ¬í˜„ì„ ì‰½ê²Œ í•˜ì˜€ìŠµë‹ˆë‹¤.\n","2. Embedding, positional encoding, transformer layerë¥¼ ê±°ì¹˜ê³  ë‚œ í›„ ë§ˆì§€ë§‰ labelì„ ì˜ˆì¸¡í•˜ê¸° ìœ„í•´ ì‚¬ìš©í•œ ê°’ì€ `x[:, 0]`ìž…ë‹ˆë‹¤. ê¸°ì¡´ì˜ RNNì—ì„œëŠ” padding tokenì„ ì œì™¸í•œ ë§ˆì§€ë§‰ tokenì— í•´ë‹¹í•˜ëŠ” representationì„ ì‚¬ìš©í•œ ê²ƒê³¼ ë‹¤ë¦…ë‹ˆë‹¤. ì´ë ‡ê²Œ ì‚¬ìš©í•  ìˆ˜ ìžˆëŠ” ì´ìœ ëŠ” attention ê³¼ì •ì„ ë³´ì‹œë©´ ì²« ë²ˆì§¸ tokenì— ëŒ€í•œ representationì€ ì´í›„ì˜ ëª¨ë“  tokenì˜ ì˜í–¥ì„ ë°›ìŠµë‹ˆë‹¤. ì¦‰, ì²« ë²ˆì§¸ token ë˜í•œ ì „ì²´ ë¬¸ìž¥ì„ ëŒ€ë³€í•˜ëŠ” ì˜ë¯¸ë¥¼ ê°€ì§€ê³  ìžˆë‹¤ê³  í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤. ê·¸ëž˜ì„œ ì¼ë°˜ì ìœ¼ë¡œ Transformerë¥¼ text ë¶„ë¥˜ì— ì‚¬ìš©í•  ë•ŒëŠ” ì´ì™€ ê°™ì€ ë°©ì‹ìœ¼ë¡œ êµ¬í˜„ë©ë‹ˆë‹¤."]},{"cell_type":"markdown","metadata":{"id":"QDq05OlAb2lB"},"source":["## í•™ìŠµ\n","\n","í•™ìŠµí•˜ëŠ” ì½”ë“œëŠ” ê¸°ì¡´ ì‹¤ìŠµë“¤ê³¼ ë™ì¼í•˜ê¸° ë•Œë¬¸ì— ë§ˆì§€ë§‰ ê²°ê³¼ë§Œ ì‚´íŽ´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YHVVsWBPQmnv"},"outputs":[],"source":["from torch.optim import Adam\n","\n","lr = 0.001\n","model = model.to('cuda')\n","loss_fn = nn.BCEWithLogitsLoss()\n","\n","optimizer = Adam(model.parameters(), lr=lr)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r88BALxO1zc1"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","\n","def accuracy(model, dataloader):\n","  cnt = 0\n","  acc = 0\n","\n","  for data in dataloader:\n","    inputs, labels = data\n","    inputs, labels = inputs.to('cuda'), labels.to('cuda')\n","\n","    preds = model(inputs)\n","    # preds = torch.argmax(preds, dim=-1)\n","    preds = (preds > 0).long()[..., 0]\n","\n","    cnt += labels.shape[0]\n","    acc += (labels == preds).sum().item()\n","\n","  return acc / cnt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":775},"executionInfo":{"elapsed":53654,"status":"error","timestamp":1743318016738,"user":{"displayName":"ì¡°ìŠ¹í˜","userId":"15759752471844115325"},"user_tz":-540},"id":"al_b56TYRILq","outputId":"ecc7b1fa-5648-4277-da25-8fc436144ff0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch   0 | Train Loss: 7.381510192528367\n","=========> Train acc: 1.000 | Test acc: 1.000\n","Epoch   1 | Train Loss: 0.004828333854675293\n","=========> Train acc: 1.000 | Test acc: 1.000\n","Epoch   2 | Train Loss: 1.4901161193847656e-08\n","=========> Train acc: 1.000 | Test acc: 1.000\n","Epoch   3 | Train Loss: 1.4901161193847656e-08\n","=========> Train acc: 1.000 | Test acc: 1.000\n","Epoch   4 | Train Loss: 1.4901161193847656e-08\n","=========> Train acc: 1.000 | Test acc: 1.000\n","Epoch   5 | Train Loss: 1.4901161193847656e-08\n","=========> Train acc: 1.000 | Test acc: 1.000\n","Epoch   6 | Train Loss: 1.4901161193847656e-08\n","=========> Train acc: 1.000 | Test acc: 1.000\n","Epoch   7 | Train Loss: 2.8049244704675402e-08\n","=========> Train acc: 1.000 | Test acc: 1.000\n","Epoch   8 | Train Loss: 1.4901161193847656e-08\n","=========> Train acc: 1.000 | Test acc: 1.000\n","Epoch   9 | Train Loss: 1.4901161193847656e-08\n","=========> Train acc: 1.000 | Test acc: 1.000\n","Epoch  10 | Train Loss: 1.4901161193847656e-08\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-d4e5ab8584c3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"=========> Train acc: {train_acc:.3f} | Test acc: {test_acc:.3f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-cf502b42c7d3>\u001b[0m in \u001b[0;36maccuracy\u001b[0;34m(model, dataloader)\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-2-fb258e4edfe8>\u001b[0m in \u001b[0;36mcollate_fn\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mtexts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m   \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m   \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2885\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_target_context_manager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2886\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_input_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2887\u001b[0;31m             \u001b[0mencodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mall_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2888\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtext_target\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2889\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_target_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   2973\u001b[0m                 )\n\u001b[1;32m   2974\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2975\u001b[0;31m             return self.batch_encode_plus(\n\u001b[0m\u001b[1;32m   2976\u001b[0m                 \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2977\u001b[0m                 \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   3175\u001b[0m         )\n\u001b[1;32m   3176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3177\u001b[0;31m         return self._batch_encode_plus(\n\u001b[0m\u001b[1;32m   3178\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3179\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens)\u001b[0m\n\u001b[1;32m    537\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_special_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_special_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m         encodings = self._tokenizer.encode_batch(\n\u001b[0m\u001b[1;32m    540\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["n_epochs = 5\n","\n","for epoch in range(n_epochs):\n","  total_loss = 0.\n","  model.train()\n","  for data in train_loader:\n","    model.zero_grad()\n","    inputs, labels = data\n","    inputs, labels = inputs.to('cuda'), labels.to('cuda').float()\n","\n","    preds = model(inputs)[..., 0]\n","    loss = loss_fn(preds, labels)\n","    loss.backward()\n","    optimizer.step()\n","\n","    total_loss += loss.item()\n","\n","  print(f\"Epoch {epoch:3d} | Train Loss: {total_loss}\")\n","\n","  with torch.no_grad():\n","    model.eval()\n","    train_acc = accuracy(model, train_loader)\n","    test_acc = accuracy(model, test_loader)\n","    print(f\"=========> Train acc: {train_acc:.3f} | Test acc: {test_acc:.3f}\")"]},{"cell_type":"code","source":[],"metadata":{"id":"u-I121osywgW"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1vh4ALpRVicq9hdonHYiLTlsPhY6LLZAE","timestamp":1743433609631}],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":0}